{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOE5kqqpdAHps/lE49bLHBK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d28ca0a313e4effb5f3543a60d90303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f06f208fb274167a43d40208281b0f5",
              "IPY_MODEL_8987ddc177f042c2bf69a3a406c06cb7",
              "IPY_MODEL_51be03976f464de084ddc4a5c981cd60"
            ],
            "layout": "IPY_MODEL_af9ba67d55d24c779251fc9f04d24631"
          }
        },
        "2f06f208fb274167a43d40208281b0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac2271b4f51247d0a6cf2c2ae555cc82",
            "placeholder": "​",
            "style": "IPY_MODEL_b77a58e297d14d4fbd5e74e649f2ef55",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "8987ddc177f042c2bf69a3a406c06cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e572f7b5ebc468fb83f02b5fb60fcae",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9244c07a848145f99dbed23cbfc91861",
            "value": 440473133
          }
        },
        "51be03976f464de084ddc4a5c981cd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fee5a993f35d4a42989ed75c0ba9955e",
            "placeholder": "​",
            "style": "IPY_MODEL_d7efd2626bde4a7193b0416f9d6c94e5",
            "value": " 440M/440M [00:03&lt;00:00, 126MB/s]"
          }
        },
        "af9ba67d55d24c779251fc9f04d24631": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac2271b4f51247d0a6cf2c2ae555cc82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b77a58e297d14d4fbd5e74e649f2ef55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e572f7b5ebc468fb83f02b5fb60fcae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9244c07a848145f99dbed23cbfc91861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fee5a993f35d4a42989ed75c0ba9955e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7efd2626bde4a7193b0416f9d6c94e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c78df9d1b3c436b9655ea0d0ceab84d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed1e0257bfcc44a18b98d8e348f63b29",
              "IPY_MODEL_729c1d4dcc994bfe8540087ea9e6f9f2",
              "IPY_MODEL_9dd79698eb6645aba7b6d691fdcc23fc"
            ],
            "layout": "IPY_MODEL_39436ee4e4a14249a56a05ebb1854b57"
          }
        },
        "ed1e0257bfcc44a18b98d8e348f63b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36861ba77ca74b9cb3d7884f7a2eea0d",
            "placeholder": "​",
            "style": "IPY_MODEL_6f91cd3fbe364b0aa4952a48d021aac7",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "729c1d4dcc994bfe8540087ea9e6f9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4a81572d1d34e81b6c274fce509f019",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46e2e077b238409ea3441fbbf27b7f61",
            "value": 231508
          }
        },
        "9dd79698eb6645aba7b6d691fdcc23fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05e79eb9b9c644c8b4e0e3cb1951a022",
            "placeholder": "​",
            "style": "IPY_MODEL_57a7133414a0490a9e4efd8013f9ce4d",
            "value": " 232k/232k [00:00&lt;00:00, 4.00MB/s]"
          }
        },
        "39436ee4e4a14249a56a05ebb1854b57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36861ba77ca74b9cb3d7884f7a2eea0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f91cd3fbe364b0aa4952a48d021aac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4a81572d1d34e81b6c274fce509f019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e2e077b238409ea3441fbbf27b7f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "05e79eb9b9c644c8b4e0e3cb1951a022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a7133414a0490a9e4efd8013f9ce4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "271e1e4a4d3b4bfdb605320e87003e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98bafa726c0d483aac6c141eb31302b1",
              "IPY_MODEL_49b6963c345d49449cb620295b0963a9",
              "IPY_MODEL_6ac53e64146a4e7ca92dfdb7a1531205"
            ],
            "layout": "IPY_MODEL_d59496466c1f404496aae9c2eca778b5"
          }
        },
        "98bafa726c0d483aac6c141eb31302b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ca4841cafb408f8e6f094722ff3e80",
            "placeholder": "​",
            "style": "IPY_MODEL_282dfa72472743c38106d120803e3718",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "49b6963c345d49449cb620295b0963a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6b0bd3f2dd64725943003d086b96ce4",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f69c5f09ab45403685645e9df5e65558",
            "value": 28
          }
        },
        "6ac53e64146a4e7ca92dfdb7a1531205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_787aabefd6174e288c0ea40fa182e6cd",
            "placeholder": "​",
            "style": "IPY_MODEL_2d97573887994e09b18b83a67ce93a75",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.28kB/s]"
          }
        },
        "d59496466c1f404496aae9c2eca778b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21ca4841cafb408f8e6f094722ff3e80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "282dfa72472743c38106d120803e3718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6b0bd3f2dd64725943003d086b96ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f69c5f09ab45403685645e9df5e65558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "787aabefd6174e288c0ea40fa182e6cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d97573887994e09b18b83a67ce93a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "399aa888f5e143b38b930ae6c8460689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_371d1d618c84460c85c920ff86391494",
              "IPY_MODEL_3810148fca8c4c13b9bc1218659400d4",
              "IPY_MODEL_45e7e1aac20b4d2dabc987d402c94c80"
            ],
            "layout": "IPY_MODEL_2a1ea1f814a94fd8a61b164a3da646d0"
          }
        },
        "371d1d618c84460c85c920ff86391494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ab116b075d148ab8791bb5aaa128c93",
            "placeholder": "​",
            "style": "IPY_MODEL_c5b7714a55fe4924aeab088b41e1c87f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "3810148fca8c4c13b9bc1218659400d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6ed0fcb6e964d9aa1e837e23e00756f",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8483061b0f84c7e95883870df27e405",
            "value": 570
          }
        },
        "45e7e1aac20b4d2dabc987d402c94c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_338f9e78fa2c46fc8b88ac5d0f1f9751",
            "placeholder": "​",
            "style": "IPY_MODEL_735c3763ec514e49857ed75bbdb9f6f8",
            "value": " 570/570 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "2a1ea1f814a94fd8a61b164a3da646d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab116b075d148ab8791bb5aaa128c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b7714a55fe4924aeab088b41e1c87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6ed0fcb6e964d9aa1e837e23e00756f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8483061b0f84c7e95883870df27e405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "338f9e78fa2c46fc8b88ac5d0f1f9751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "735c3763ec514e49857ed75bbdb9f6f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LawrenceLLY/GNN_Pun_Detection/blob/main/BERT_GNN_MTL_Control_BiLSTM_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Packages"
      ],
      "metadata": {
        "id": "jDAHp7MPmYkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyOMOATLlCUs",
        "outputId": "31ebc7f3-18d6-4366-b296-9d41314c1c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.22.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.1+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.9/884.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910476 sha256=299388887126e6399eb095ec574c3a6277543b06f2499cea99adfe58feebe4ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "2023-05-04 20:17:23.148096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-04 20:17:24.298916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n",
        "# The same as Torch version and CUDA version (torch.__version__ is 2.0.0+cu118)\n",
        "!pip install torch-geometric\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "print(torch.__version__)\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "# import numpy as np\n",
        "from transformers import BertModel, BertTokenizer, BertConfig\n",
        "import csv\n",
        "import spacy\n",
        "import networkx as nx\n",
        "#from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "import xml.etree.ElementTree as ET\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus.reader.wordnet import Synset\n",
        "from nltk.wsd import lesk\n",
        "import nltk\n",
        "import pickle\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvK4NZGBltC2",
        "outputId": "88e5b6eb-b696-4325-cb8c-cc398ca903db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm') # DO NOT split a sentence for more 1 time\n",
        "# If spliting a sentence more than 1 time, the result may different from the same split\n",
        "model_path = \"/content/drive/My Drive/my_GNN_MTL_PT_model_v1.pt\"  # Choose your desired path and filename\n",
        "# Model parameters\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "OuCaASlnTGzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "dO6z-dOehyVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the semeval2017_task7 dataset"
      ],
      "metadata": {
        "id": "dbkiSdvYPifm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fsmh_cLevBI",
        "outputId": "3ce8cd49-d3ee-457d-afdf-2415a139c855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-04 04:19:18--  https://alt.qcri.org/semeval2017/task7/data/uploads/semeval2017_task7.tar.xz\n",
            "Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.231\n",
            "Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748424 (731K) [application/x-xz]\n",
            "Saving to: ‘semeval2017_task7.tar.xz.3’\n",
            "\n",
            "semeval2017_task7.t 100%[===================>] 730.88K   717KB/s    in 1.0s    \n",
            "\n",
            "2023-05-04 04:19:20 (717 KB/s) - ‘semeval2017_task7.tar.xz.3’ saved [748424/748424]\n",
            "\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/              semeval2017_task7.tar.xz    semeval2017_task7.tar.xz.3\n",
            "\u001b[01;34msample_data\u001b[0m/        semeval2017_task7.tar.xz.1\n",
            "\u001b[01;34msemeval2017_task7\u001b[0m/  semeval2017_task7.tar.xz.2\n"
          ]
        }
      ],
      "source": [
        "!wget https://alt.qcri.org/semeval2017/task7/data/uploads/semeval2017_task7.tar.xz\n",
        "!tar -xf semeval2017_task7.tar.xz\n",
        "#!tar -xvf semeval2017_task7.tar.xz\n",
        "#%cd semeval2017_task7/\n",
        "#%cd ..\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## homographic"
      ],
      "metadata": {
        "id": "cKmhgmUo_hf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'semeval2017_task7/data/test/subtask1-homographic-test.xml'\n",
        "\n",
        "mytree = ET.parse(f)\n",
        "myroot = mytree.getroot()\n",
        "\n",
        "puns_hom = []\n",
        "for item in myroot.findall('./text'):\n",
        "    dict1 = {}\n",
        "    dict1[item.attrib['id']] = {}\n",
        "    for child in item:\n",
        "        idd = child.attrib['id']\n",
        "        dict1[item.attrib['id']][idd] = child.text\n",
        "    for pun in dict1.values():\n",
        "        puns_hom.append([pun[x].replace(u'\\xa0', '_') for x in pun])\n",
        "\n",
        "print(puns_hom[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUsbD-WD6h1q",
        "outputId": "c81d1c00-4fc6-493e-d58e-8bc25d808c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['They', 'hid', 'from', 'the', 'gunman', 'in', 'a', 'sauna', 'where', 'they', 'could', 'sweat', 'it', 'out', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gold_hom = []\n",
        "with open('semeval2017_task7/data/test/subtask1-homographic-test.gold', 'r') as fin:\n",
        "    for row in fin:\n",
        "        gold_hom.append(int(row.strip().split('\\t')[1]))\n",
        "print(gold_hom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB3jjujn7WZY",
        "outputId": "faf5d6a7-f65c-433e-f66f-85bb3f34a34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location_hom = [-10000 for _ in range(len(puns_hom))]\n",
        "with open('semeval2017_task7/data/test/subtask2-homographic-test.gold', 'r') as fin:\n",
        "    for row in fin:\n",
        "        # The default is start from 1\n",
        "        pun_index = int(row.strip().split('\\t')[1].split('_')[2]) - 1\n",
        "        location_hom[int(row.strip().split('\\t')[1].split('_')[1]) - 1] = pun_index\n",
        "print(location_hom)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQOiNbDU9W4e",
        "outputId": "a2e187f0-9f65-4fb9-8e04-7b0631676e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 8, 6, 4, 14, -10000, 13, 16, 10, 12, 8, -10000, -10000, 13, 14, 12, -10000, 7, 2, 16, 8, 14, 6, 6, 4, 14, 9, 4, 3, 7, 11, -10000, 3, 14, -10000, -10000, -10000, 8, 19, -10000, 15, -10000, 7, 11, 5, 8, 12, 9, -10000, 8, 7, 18, 7, -10000, 11, -10000, -10000, 14, -10000, 19, 9, 8, 9, -10000, -10000, -10000, 14, -10000, 10, 8, 20, 9, 8, 8, 12, 6, 13, 17, -10000, 4, 8, 5, 3, -10000, 14, 19, 12, 10, -10000, 2, 14, 9, 13, -10000, -10000, 11, 5, 14, 9, 19, -10000, -10000, 6, 16, -10000, 8, 16, 9, 9, 10, -10000, 6, 10, -10000, 10, 13, 4, -10000, 6, 11, -10000, -10000, -10000, 6, 16, 0, 15, 15, -10000, 10, -10000, 3, 11, -10000, 9, 14, 4, 14, 11, 11, 16, 16, -10000, 10, 10, -10000, 7, 14, 8, 7, 13, 20, 1, 8, -10000, 9, 7, -10000, 20, 8, 11, 7, 12, 11, 12, -10000, 5, -10000, 7, 22, -10000, 9, 7, 16, 7, -10000, 7, 14, 9, -10000, 16, -10000, 6, -10000, 11, 12, -10000, 21, 10, -10000, 10, -10000, 13, -10000, 6, 13, 12, 2, 20, -10000, 12, 11, 6, 16, -10000, 14, 7, 12, 16, -10000, -10000, -10000, -10000, 12, -10000, -10000, 11, 4, 11, 10, 18, 6, 8, 10, 7, 9, 13, 9, 2, 3, 13, -10000, 25, 5, 8, 11, 7, 2, -10000, 3, -10000, -10000, 10, -10000, 9, 8, 3, -10000, -10000, 8, 12, -10000, -10000, 15, 9, 3, -10000, 13, 12, 11, 10, 4, -10000, -10000, 9, 11, 8, 11, 6, 12, -10000, 7, 8, 13, -10000, 12, 6, 17, 10, -10000, 14, 18, 21, 13, 18, 9, 3, 16, 6, -10000, -10000, 11, 17, 10, -10000, 6, -10000, 10, -10000, 4, 13, -10000, 8, 6, 12, -10000, -10000, 34, -10000, 16, -10000, -10000, 11, -10000, 20, 13, 19, 16, 12, 6, -10000, 9, 10, 10, 11, -10000, 12, 13, 3, 7, -10000, -10000, -10000, 9, -10000, -10000, -10000, 8, 16, -10000, 9, 13, -10000, 4, 9, 11, -10000, 18, 6, 10, -10000, -10000, 4, 2, 14, 7, 6, 7, 7, 9, 6, 10, 14, 13, 17, 13, 11, 10, 10, -10000, 5, -10000, 20, 10, 10, -10000, -10000, 11, -10000, 19, 9, 7, 9, -10000, -10000, 8, 12, 10, 9, 12, 9, 4, 9, 9, 7, 17, 12, -10000, 12, 8, 12, 13, 11, 17, 11, 5, -10000, 10, 9, -10000, 7, 29, -10000, 12, 3, 2, 13, 8, -10000, -10000, 15, 19, 11, 15, -10000, 6, -10000, 9, 7, 15, 14, 19, 9, 12, -10000, -10000, 5, 13, -10000, 8, 8, 8, 9, 3, -10000, 10, 28, -10000, 16, -10000, 16, 7, -10000, 10, -10000, -10000, 11, 9, -10000, -10000, -10000, 12, 7, 9, -10000, -10000, 11, 9, -10000, 11, -10000, -10000, 22, -10000, -10000, -10000, -10000, 16, 10, -10000, 14, 8, 8, -10000, 14, 7, 20, 8, -10000, 11, 3, -10000, 6, -10000, 10, 7, 11, 3, -10000, 19, -10000, 9, 13, 16, 5, 7, -10000, 8, 4, -10000, 21, 10, 15, 9, -10000, 9, -10000, -10000, -10000, 8, -10000, 16, -10000, 8, 5, 9, 11, -10000, 7, -10000, 11, -10000, -10000, 3, 5, -10000, -10000, 6, -10000, 4, -10000, 8, 13, 7, 7, 33, 6, 20, 5, 9, -10000, 5, 3, 13, 9, 10, 8, 8, 3, -10000, -10000, -10000, 5, -10000, 9, 13, -10000, 7, 0, 7, 9, 13, 6, 10, 10, 10, -10000, 13, 12, 16, 10, 8, -10000, 16, -10000, 4, 6, 7, -10000, 10, 9, -10000, 12, 6, 13, 15, 5, 8, 11, 15, -10000, 9, -10000, 18, 5, -10000, -10000, 7, 12, -10000, -10000, 3, -10000, 17, 7, -10000, -10000, -10000, 11, 12, 5, 3, 3, 10, 8, -10000, 0, 12, 10, 7, 8, -10000, -10000, -10000, 17, 9, 10, 10, -10000, 7, -10000, 11, -10000, 8, -10000, 14, -10000, 2, 9, 13, -10000, -10000, -10000, 10, 13, 2, 12, 11, 6, 11, 16, 9, 17, 9, -10000, 8, 9, 9, -10000, 14, 5, 9, 12, 15, 15, 6, 5, 3, 7, 16, 6, -10000, -10000, 3, -10000, 10, 10, 5, 14, 4, 12, 6, 43, 9, 7, -10000, 11, 10, 16, 8, 17, -10000, 7, 10, 8, -10000, 14, 15, -10000, 23, 7, 4, 4, 13, 8, 2, 9, 9, -10000, -10000, -10000, -10000, 12, -10000, -10000, 14, 8, 8, 16, -10000, 11, 14, -10000, -10000, 2, 8, 8, 15, -10000, 9, 7, 10, -10000, 21, 13, -10000, -10000, 6, 10, -10000, 7, -10000, 16, 6, -10000, 15, -10000, 10, 6, 3, -10000, 6, 8, 13, 10, 12, 12, 13, 12, 9, 15, -10000, -10000, 3, 22, -10000, -10000, 11, 11, 18, -10000, -10000, 18, -10000, 22, -10000, 32, 10, 3, 12, 17, 14, 6, -10000, -10000, 9, -10000, -10000, 10, 14, -10000, 4, -10000, 9, 8, 2, 9, 7, -10000, 11, 16, 7, -10000, 11, 5, -10000, 10, -10000, 14, 10, 9, -10000, -10000, -10000, 12, -10000, -10000, 17, 16, -10000, -10000, -10000, 10, 12, 18, 6, -10000, 9, -10000, -10000, 15, -10000, 14, 3, 8, -10000, 25, 13, -10000, -10000, 7, 11, 10, 9, 4, 16, 14, 5, -10000, 7, 16, 20, 4, 24, -10000, 25, -10000, 15, -10000, 14, 9, 8, 9, -10000, -10000, 8, 8, 9, -10000, -10000, 10, 3, -10000, 3, 1, 11, -10000, 12, 20, 4, 8, -10000, -10000, -10000, 11, -10000, 14, 5, -10000, 7, 9, 3, 6, -10000, -10000, 3, 12, 18, -10000, 7, 14, 8, 8, 6, 15, 7, 14, 16, -10000, 10, -10000, 17, 5, 9, 9, 8, -10000, 16, 12, -10000, -10000, -10000, -10000, 9, -10000, -10000, 21, 7, 5, 21, 27, 9, 8, -10000, 13, 11, -10000, 3, 16, -10000, -10000, -10000, -10000, 12, 9, 7, 2, 14, 14, 12, 17, 15, -10000, 12, -10000, 15, -10000, 0, 13, 7, 5, 7, 6, 7, -10000, 12, 10, 3, 17, -10000, 12, -10000, 9, 20, 10, -10000, 9, 13, 13, 10, 25, 20, 18, 10, 16, 15, -10000, 18, 16, -10000, -10000, 10, -10000, 11, -10000, 10, 14, 7, 17, 8, -10000, 9, 4, 16, 12, 17, 14, 6, 11, 6, -10000, 10, 2, 11, 7, -10000, 15, 19, -10000, 7, 10, 10, -10000, 15, 10, 12, 6, -10000, 8, 7, -10000, 6, 9, 11, 11, -10000, 12, 4, 6, 7, 9, 17, 1, 3, 5, 42, 8, 8, -10000, 14, 12, 8, 12, -10000, 13, 14, 15, 21, 12, -10000, 17, 16, 16, 11, -10000, -10000, 7, 7, 12, 3, 13, 11, 10, 11, 9, 6, -10000, 3, -10000, 17, 0, 11, 11, -10000, -10000, -10000, -10000, -10000, -10000, 12, 20, 12, 7, -10000, 5, -10000, 21, -10000, 3, 8, 2, 7, 10, 4, -10000, 15, 6, -10000, 43, 27, -10000, 13, 17, 9, -10000, 5, 10, 6, -10000, 13, -10000, 11, 9, 12, 11, 15, -10000, -10000, 13, 24, 9, -10000, 1, 25, -10000, -10000, 9, 17, -10000, -10000, 19, -10000, 9, 4, 9, 7, -10000, 8, -10000, 15, 12, 7, 7, 23, 8, 3, -10000, 13, -10000, -10000, 20, 2, 7, 8, 12, -10000, 14, 9, 6, -10000, 18, 17, 19, 17, 7, 13, 3, -10000, -10000, 10, 14, 8, 3, 15, 12, 15, 18, 9, -10000, 6, 5, -10000, 9, -10000, -10000, 3, -10000, -10000, 17, -10000, 4, 3, 17, 8, 13, -10000, 8, -10000, 12, -10000, -10000, 15, -10000, 16, 20, -10000, 5, -10000, 11, -10000, 15, 15, -10000, 11, -10000, 13, 18, 4, 11, -10000, 3, -10000, 3, 3, 3, 6, 7, 12, 7, -10000, 17, -10000, 9, 19, -10000, -10000, 10, -10000, 17, 26, 13, 10, 22, 6, 14, -10000, 15, 9, 10, 8, 10, 5, -10000, 11, 19, 6, 5, 15, 17, -10000, 6, 8, 14, 5, -10000, 8, 15, -10000, -10000, 7, 5, 14, -10000, 12, 7, 11, -10000, 2, -10000, 9, 12, 13, -10000, 10, 7, -10000, 3, 7, 24, 7, 9, 5, -10000, 13, 6, -10000, 14, -10000, -10000, 6, 12, -10000, 8, 13, 7, 15, 17, 7, 7, -10000, 16, 17, -10000, 9, -10000, 8, -10000, 11, 16, 6, 15, 11, 11, 9, 15, 3, -10000, 12, 11, 18, 9, 6, -10000, 18, 3, 14, 8, 8, 25, 10, -10000, -10000, 9, 11, 9, 11, 15, 12, 1, -10000, -10000, 9, 8, 13, 3, -10000, 11, 3, 7, -10000, 13, 11, 14, 11, 7, -10000, 19, -10000, 7, -10000, 6, 9, 8, 7, 11, 9, 5, -10000, -10000, 18, 12, -10000, 9, 14, -10000, -10000, 3, 12, -10000, 8, -10000, 18, -10000, 15, -10000, 19, 9, 9, 9, 8, 12, -10000, 14, 19, 9, 7, 2, 7, 25, -10000, 9, 9, 9, 3, 16, 8, 14, 9, 10, 9, 8, 13, -10000, 10, -10000, -10000, 5, -10000, 19, 8, 15, 14, 2, -10000, -10000, 5, -10000, 0, 3, 13, -10000, 17, 16, -10000, 16, 22, -10000, -10000, 3, 4, 10, -10000, 12, -10000, -10000, 13, 14, -10000, 21, -10000, 9, 0, 11, -10000, 14, 4, 0, 17, -10000, -10000, 10, 13, -10000, -10000, 22, 15, 11, -10000, 8, -10000, 11, 6, 3, 9, 21, 8, 14, 14, 6, 6, 2, 9, 5, 4, 3, 13, 13, 17, 9, 6, 8, -10000, 13, -10000, -10000, 8, 17, -10000, 7, -10000, 12, 13, -10000, 16, 9, -10000, 8, 5, 9, 14, 11, 8, 13, 8, 16, 9, 9, 15, 13, -10000, 3, -10000, 10, 3, 10, 6, 5, 8, 9, 6, 15, 9, 11, 13, 8, -10000, 12, -10000, 8, 10, -10000, -10000, 5, 14, -10000, -10000, -10000, 9, 15, -10000, -10000, -10000, -10000, 5, 12, 28, 8, 15, 12, 19, 6, 10, 3, 19, -10000, 9, -10000, -10000, -10000, -10000, -10000, 19, -10000, 3, 14, -10000, 9, 10, 15, 10, -10000, -10000, -10000, 11, 6, 6, 14, 12, -10000, 10, 13, 6, -10000, 8, 13, 3, 9, 9, 7, -10000, 9, 14, -10000, -10000, 10, 10, 14, 11, -10000, -10000, -10000, 5, -10000, -10000, 10, 6, 9, 10, -10000, 5, 21, 7, 12, 3, 5, 14, 8, 9, 11, 9, 11, -10000, 7, -10000, 15, 4, -10000, -10000, -10000, 9, 9, -10000, -10000, -10000, 9, 11, 12, 0, -10000, 8, -10000, 13, -10000, -10000, 9, 6, 12, -10000, 6, 3, 14, 10, -10000, 11, 9, 16, 25, 16, 3, 15, 6, 4, 12, 14, -10000, -10000, 12, 2, 4, -10000, 6, 13, 12, 7, 6, -10000, 5, -10000, 12, 14, -10000, -10000, -10000, -10000, -10000, 12, 10, 10, 9, 15, -10000, 11, -10000, 11, 3, 6, 3, -10000, 10, -10000, -10000, 19, -10000, 11, -10000, -10000, 9, 18, 14, 10, 19, -10000, 13, 11, 3, 13, 12, 10, 12, 7, 4, 6, 15, 16, -10000, 8, 10, 7, -10000, 9, 9, -10000, 12, -10000, 9, -10000, -10000, 12, -10000, 8, 4, 15, 8, -10000, 4, 12, 8, -10000, -10000, 9, 6, 23, -10000, 8, 16, 8, 14, 4, 7, -10000, 6, -10000, 14, 13, 14, 11, 9, 8, 5, -10000, -10000, 10, 11, 11, 16, -10000, -10000, -10000, 4, 12, 9, 16, 8, 10, 1, 11, -10000, 11, -10000, 6, 7, 8, 9, 4, 7, 10, 14, 16, -10000, 15, 9, -10000, 11, 8, -10000, 12, -10000, 12, 6, 17, 8, 8, 12, 2, 3, 6, 21, 6, 17, 10, 12, 19, 7, -10000, -10000, 8, -10000, 8, -10000, 10, 12, 8, 23, -10000, 12, 7, 13, 15, 14, 9, -10000, -10000, 12, 18, 12, 12, 9, 12, -10000, -10000, 8, 7, -10000, 16, -10000, -10000, -10000, -10000, -10000, -10000, -10000, -10000, 8, -10000, 13, 7, 13, -10000, -10000, -10000, -10000, 14, 12, -10000, 5, -10000, 10, 16, -10000, 10, 23, 8, 12, 18, 11, 7, 10, 2, 11, 2, -10000, 13, 8, 15, -10000, 11, -10000, 4, -10000, 13, 8, 8, 3, 10, 9, 18, 14, 10, -10000, -10000, 15, 16, 11, 21, 15, 22, 10, 8, -10000, 5, -10000, 5, 9, 7, 19, 3, -10000, 13, 9, 4, 8, 12, -10000, 3, 10, 13, 9, 14, 7, 7, -10000, 11, 13, -10000, 4, 9, 8, 13, 13, 2, 15, 4, -10000, 10, 6, 5, -10000, -10000, 6, -10000, 10, 4, -10000, 9, 6, 3, -10000, 7, 16, 10, 14, 4, -10000, 18, 7, 9, 10, 15, 12, -10000, 2, -10000, 17, 8, 10, -10000, 2, 5, 14, 8, -10000, 13, 21, 11, -10000, 13, 5, 1, 20, 9, 1, 3, -10000, 6, 3, -10000, -10000, 15, -10000, 12, 11, 7, 0, -10000, 8, 3, 18, 10, -10000, 16, 16, 7, 7, 10, 9, 12, 14, 11, 11, -10000, 12, 8, 20, -10000, -10000, 11, 13, 7, 7, 10, 13, -10000, 10, 5, 13, -10000, 9, 17, 14, -10000, 3, 2, 8, 14, -10000, 17, 7, -10000, 8, 11, 3, 4, 13, 8, -10000, 7, 10, 8, -10000, 13, 14, -10000, 8, 16, -10000, 18, 6, 15, 6, 17, 18, 6, -10000, 12, 12, 12, 9, -10000, 13, -10000, -10000, 10, 9, 18, 11, 18, -10000, 12, 10, 16, 9, 6, 0, 7, -10000, -10000, 3, 11, -10000, 0, -10000, 10, 12, -10000, 9, -10000, 5, 10, 18, 12, 9, -10000, 13, 16, 10, 8, -10000, 7, 13, 10, -10000, -10000, 11, -10000, 10, 9, 3, 9, 9, 11, 17, -10000, 9, 3, 12, 15, 3, -10000, -10000, -10000, 3, 10, -10000, -10000, -10000, -10000, 3, -10000, -10000, 8, -10000, 10, -10000, 7, -10000, -10000, 3, 9, 2, 11, -10000, 21, 3, 3, -10000, 5, 14, -10000, -10000, 2, -10000, 3, -10000, 10, -10000, -10000, 8, 18, 4, -10000, 11, -10000, 5, -10000, 12, 2, 10, 8, 8, 19, 11, 7, -10000, 8, 9, -10000, -10000, 7, 12, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(puns_hom) == len(gold_hom)\n",
        "assert len(gold_hom) == len(location_hom)"
      ],
      "metadata": {
        "id": "hDWfG2mr_Exj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## heterographic"
      ],
      "metadata": {
        "id": "rFRjHj_b_2YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = 'semeval2017_task7/data/test/subtask1-heterographic-test.xml'\n",
        "\n",
        "mytree = ET.parse(f)\n",
        "myroot = mytree.getroot()\n",
        "\n",
        "puns_het = []\n",
        "for item in myroot.findall('./text'):\n",
        "    dict1 = {}\n",
        "    dict1[item.attrib['id']] = {}\n",
        "    for child in item:\n",
        "        idd = child.attrib['id']\n",
        "        dict1[item.attrib['id']][idd] = child.text\n",
        "    for pun in dict1.values():\n",
        "        puns_het.append([pun[x].replace(u'\\xa0', '_') for x in pun])\n",
        "\n",
        "print(puns_het[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD2D5e_x_xUK",
        "outputId": "23f404bb-ffde-4367-802b-c94fb4876d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", \"'\", 'I', \"'\", 'm', 'halfway', 'up', 'a', 'mountain', ',', \"'\", \"'\", 'Tom', 'alleged', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gold_het = []\n",
        "with open('semeval2017_task7/data/test/subtask1-heterographic-test.gold', 'r') as fin:\n",
        "    for row in fin:\n",
        "        gold_het.append(int(row.strip().split('\\t')[1]))\n",
        "print(gold_het)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DVjLUSz_-bs",
        "outputId": "6387a3e1-a063-4d7c-c827-58c0acd31a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location_het = [-10000 for _ in range(len(puns_het))]\n",
        "with open('semeval2017_task7/data/test/subtask2-heterographic-test.gold', 'r') as fin:\n",
        "    for row in fin:\n",
        "        # The default is start from 1\n",
        "        pun_index = int(row.strip().split('\\t')[1].split('_')[2]) - 1\n",
        "        location_het[int(row.strip().split('\\t')[1].split('_')[1]) - 1] = pun_index\n",
        "print(location_het)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryJLOM-U_-Uc",
        "outputId": "fcc42701-dc61-4dc5-c3c7-69378fe9d7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13, 12, -10000, 10, 4, -10000, 5, 3, 5, 7, 12, 11, 23, -10000, 2, 10, 6, -10000, 15, 15, -10000, 16, 12, 5, 6, 11, -10000, 9, -10000, -10000, 13, -10000, 23, 15, 13, 16, -10000, 12, 14, 5, -10000, 10, 9, 14, 7, 9, 7, -10000, 7, 3, 10, -10000, -10000, 28, 10, 6, -10000, 13, 12, 15, -10000, 12, -10000, 15, 17, 7, 6, -10000, 13, 26, 13, 3, 12, 12, 16, 11, 4, 6, -10000, 10, -10000, 10, 10, -10000, -10000, 9, 11, -10000, 10, 5, -10000, 7, -10000, 6, 18, 8, 3, -10000, 11, 6, 15, 5, -10000, 4, 3, 14, 7, 13, -10000, -10000, 7, 10, 12, -10000, -10000, 6, 30, -10000, 6, -10000, 14, 6, 14, 11, 6, 39, -10000, -10000, 11, -10000, 16, -10000, 8, 15, -10000, 24, 23, 17, -10000, 13, 5, 11, 14, 3, 13, -10000, -10000, 4, 6, 8, -10000, -10000, 12, 14, -10000, 1, 3, 6, 12, -10000, -10000, 4, -10000, -10000, 17, 25, -10000, 9, -10000, -10000, -10000, 19, -10000, 6, -10000, -10000, 6, -10000, 16, -10000, 22, -10000, 13, 8, 12, 19, 17, 16, 9, -10000, 14, -10000, 14, 5, 10, 12, 5, -10000, -10000, 16, 6, 6, 8, 11, -10000, 11, -10000, 8, 16, -10000, -10000, -10000, 8, -10000, 3, 9, 9, 8, 10, -10000, 13, 21, 6, 8, -10000, -10000, 10, 10, 7, -10000, 10, -10000, -10000, 8, 10, 13, -10000, -10000, 6, 4, -10000, 5, -10000, 11, -10000, 10, 19, 10, 12, 8, 19, 12, 21, 12, -10000, -10000, 11, 8, 14, 8, 25, -10000, 9, 8, 9, -10000, 11, 14, -10000, 14, -10000, 3, -10000, -10000, -10000, 5, 21, 11, -10000, -10000, -10000, 5, 18, 13, 10, -10000, 3, 7, 11, -10000, 14, 6, 0, 21, -10000, 7, -10000, 16, 17, 7, 6, 19, 7, -10000, 7, 15, -10000, 30, 12, 16, 13, 7, 16, 2, 8, 10, 16, 11, -10000, 5, -10000, 11, 11, 5, 6, 10, -10000, 2, 7, -10000, 9, 17, 10, 10, 24, 13, 11, 8, -10000, -10000, 12, 9, 3, 14, 16, 4, 7, 17, -10000, 10, 37, -10000, 13, 6, -10000, 13, 17, 9, -10000, 9, 10, 12, 7, 10, 15, 24, -10000, -10000, 7, 32, -10000, -10000, 11, 8, 19, -10000, 18, -10000, 8, 15, 11, 20, -10000, 9, -10000, 7, 15, 13, 17, -10000, 16, -10000, 7, 14, -10000, 11, -10000, 7, 3, 16, -10000, -10000, -10000, -10000, 7, 18, -10000, 9, 10, 15, 14, 18, -10000, -10000, 12, 3, 17, 18, 13, 14, 14, -10000, -10000, 11, -10000, 14, 28, 7, 11, 4, 10, -10000, -10000, 2, 31, 11, -10000, -10000, -10000, 12, -10000, 11, -10000, 5, 10, 27, -10000, -10000, 6, 10, 14, -10000, 9, 15, -10000, -10000, 8, 11, 15, 0, 9, 13, 3, 9, -10000, 7, 4, 18, 12, 7, 15, 3, 8, -10000, -10000, -10000, 2, 13, 14, 16, 31, 5, 1, -10000, 10, 14, 7, 14, 20, -10000, 4, 7, 12, 6, -10000, 4, 4, 16, 8, -10000, 15, 8, 13, 9, 6, 18, -10000, 10, 8, 7, -10000, -10000, 27, 8, -10000, 10, 2, 7, 6, 19, -10000, 4, 16, 16, 26, -10000, 30, 10, -10000, 4, -10000, -10000, -10000, 10, 17, -10000, 17, 6, 19, 13, 3, 13, 14, 6, 14, -10000, -10000, 7, 6, 6, 8, 10, 9, 9, 6, 17, -10000, 11, 12, 7, 6, 7, -10000, 22, -10000, 5, 11, -10000, -10000, -10000, 8, -10000, 10, 15, 6, -10000, -10000, 9, 7, 24, -10000, 14, 1, 16, 7, 11, 7, 4, 39, -10000, 9, 12, 20, -10000, 11, -10000, 31, -10000, -10000, 3, 8, 12, -10000, -10000, 13, 18, 16, 9, 13, 63, 6, 11, -10000, -10000, -10000, -10000, 6, 10, 7, -10000, 16, 7, -10000, 16, 1, -10000, 17, 18, 6, 9, 11, 22, 8, 3, 11, 7, -10000, 6, 18, 7, -10000, -10000, 20, 22, 10, 7, 11, -10000, 7, -10000, 18, 17, 16, 3, 13, 13, 15, -10000, -10000, -10000, 9, 10, 9, -10000, 15, 10, 13, 10, 7, 4, 3, 10, -10000, 14, 13, 13, 5, 21, -10000, -10000, 15, -10000, 10, 19, 15, 7, -10000, -10000, 10, 3, 18, 5, 11, 3, -10000, -10000, 20, 7, 8, 18, 8, -10000, 14, 6, 3, 8, -10000, 14, 9, 19, 19, -10000, -10000, 10, -10000, 10, 15, 17, 7, 10, 15, 1, 9, 7, 11, -10000, 12, -10000, -10000, 10, 17, 15, 6, -10000, 8, 10, 9, -10000, -10000, 9, -10000, 16, 9, -10000, -10000, 12, -10000, -10000, 29, 6, 10, 14, 9, 7, 20, 12, 15, 14, 11, 9, 14, 1, 2, 10, 1, -10000, -10000, 16, 7, 12, 10, -10000, 10, -10000, 7, 11, 22, -10000, 16, -10000, 12, 17, 15, 17, 12, 6, -10000, 9, -10000, -10000, 8, 5, 9, 10, 20, -10000, 13, 16, 14, -10000, -10000, 11, 12, 10, 10, 14, 10, 19, -10000, 10, -10000, -10000, 8, 18, 16, -10000, -10000, 8, -10000, 8, 24, 10, -10000, 9, 11, 9, 30, 6, 18, 9, -10000, 0, -10000, 19, -10000, 11, 13, -10000, -10000, 18, -10000, 2, 23, 15, -10000, -10000, 18, 11, 12, -10000, 7, 3, -10000, 20, 19, -10000, 15, -10000, 11, -10000, 8, -10000, 4, 12, -10000, 10, 12, -10000, 19, 13, 18, 17, -10000, 8, 4, 3, 11, -10000, -10000, 17, 4, 9, 18, -10000, 11, 7, 8, 9, 7, 8, 34, 11, 12, -10000, 13, -10000, 3, 17, -10000, 18, 16, 4, 18, 3, 20, -10000, 7, 7, 8, 3, 13, 3, 10, 6, 12, -10000, -10000, -10000, -10000, 6, 8, 5, 12, 5, -10000, 13, -10000, 11, 4, 6, -10000, -10000, 4, 7, 13, -10000, 7, -10000, 14, 9, -10000, 15, 15, 29, 6, -10000, -10000, 8, 21, 7, 0, 16, 8, 8, 19, 3, -10000, 27, -10000, -10000, -10000, -10000, -10000, 6, -10000, -10000, -10000, -10000, 8, -10000, 19, -10000, 15, 18, -10000, -10000, 9, 16, -10000, 10, 8, 14, 8, 11, 9, 13, 14, -10000, 10, 5, 11, 9, 23, -10000, 3, -10000, 9, 10, -10000, -10000, 16, 13, 0, 6, 18, 12, 12, 13, -10000, 12, 16, 21, 10, 15, -10000, 14, 12, 73, 11, 11, 13, 12, -10000, 8, -10000, 9, 9, 13, 8, 9, 13, 11, 16, 6, 10, 21, 10, 8, 18, -10000, -10000, 15, 44, -10000, 16, 10, 3, 8, 5, 9, 15, 11, 14, 16, 19, 16, 8, -10000, -10000, -10000, 12, 8, 13, -10000, 32, 21, 18, -10000, -10000, 15, 9, -10000, 16, 36, 2, 7, 15, 8, 8, 14, -10000, 13, -10000, 14, 10, 8, -10000, 25, 13, 8, 13, -10000, -10000, 7, 9, 15, 3, 12, 12, -10000, 10, -10000, 35, 22, 11, 11, 3, -10000, 4, 17, -10000, 7, 13, 15, 9, 4, 9, 12, 3, -10000, -10000, 12, 11, -10000, -10000, 7, 15, 9, 4, 10, -10000, 7, 18, 14, -10000, 9, 7, 9, -10000, 14, 21, 13, 12, 18, -10000, -10000, 3, 13, -10000, 6, -10000, 20, -10000, -10000, 8, 8, 12, -10000, 10, 8, -10000, -10000, -10000, 3, -10000, -10000, 9, 12, -10000, -10000, 8, -10000, 8, 9, 34, 10, 19, -10000, -10000, 13, 7, -10000, 15, 10, 8, 12, -10000, -10000, -10000, 9, -10000, -10000, 14, 14, 16, -10000, 8, -10000, 8, 14, 6, 8, 4, 13, 17, 12, 25, -10000, 13, -10000, -10000, 1, 13, 14, 8, 9, 8, 7, 16, -10000, 10, 2, -10000, -10000, 8, 9, 15, 6, 17, -10000, -10000, 15, 3, -10000, -10000, -10000, 25, 1, 8, -10000, 7, 11, 4, 14, -10000, -10000, 7, 15, 10, 11, 11, -10000, -10000, 30, 16, 12, -10000, -10000, 16, 11, 16, 4, -10000, 2, 12, 9, -10000, 4, 7, -10000, 10, 7, 7, -10000, 9, 14, -10000, 11, -10000, 3, -10000, 10, 15, 11, 8, 9, 8, -10000, 15, 11, 12, 14, -10000, -10000, 11, 14, 17, 5, 11, 11, 11, 15, 19, 17, 7, -10000, -10000, 7, -10000, 11, 3, 7, -10000, 22, 13, 3, 23, -10000, 9, -10000, -10000, 10, -10000, -10000, -10000, -10000, 14, 13, 7, 1, 15, 13, 5, -10000, 5, 8, 11, -10000, 8, 7, -10000, 15, -10000, -10000, 10, 9, 7, 8, 7, -10000, 9, -10000, 7, 4, -10000, 9, -10000, -10000, 5, 0, 12, 19, 8, 10, 13, -10000, 13, 12, 10, 12, 5, 7, -10000, -10000, -10000, -10000, -10000, 16, 10, 6, 14, 11, -10000, 17, 11, 4, 15, 7, 11, 17, 4, 10, 13, 7, 10, 7, 3, 13, 11, -10000, 8, 7, 9, -10000, 3, 19, 7, -10000, -10000, -10000, -10000, 7, 11, 17, -10000, 10, 10, 75, 6, 10, 17, 16, 22, -10000, 12, -10000, 14, 17, 11, -10000, 15, 15, 10, 12, 6, 11, 9, 4, -10000, -10000, 18, 13, 7, 8, -10000, 6, 12, -10000, 10, 13, -10000, 3, 7, 7, 3, 7, 26, 6, 7, -10000, 17, -10000, 9, 3, 14, 15, 9, 9, 12, 13, 5, 8, 5, 8, 6, 6, 10, 4, 9, 3, 11, 33, 15, 12, -10000, 34, 10, 25, 55, 14, 12, 1, -10000, 10, 29, -10000, 15, 5, 9, 9, 11, 14, 11, 29, 4, 8, 3, 12, 7, 12, 9, 14, -10000, 16, 21, -10000, -10000, 11, 7, -10000, -10000, -10000, -10000, 1, 2, 12, 15, 14, 9, 4, 18, 11, -10000, -10000, -10000, -10000, 11, 13, 10, -10000, 5, 13, -10000, 8, 5, 36, 6, -10000, 15, -10000, -10000, -10000, 9, 7, 7, 6, 21, 25, -10000, 8, 6, -10000, 10, -10000, -10000, 8, -10000, -10000, 13, -10000, 13, 27, -10000, -10000, 10, 5, -10000, 5, 14, 6, 2, 14, 11, -10000, -10000, 6, 8, 11, 8, 14, -10000, -10000, -10000, 20, 11, -10000, 28, 6, 19, 15, -10000, 13, -10000, 18, 8, 16, -10000, 5, -10000, 9, 9, 14, 7, 15, 9, 14, 8, -10000, 32, -10000, -10000, 11, -10000, 7, 15, 5, 13, -10000, 11, 11, 18, -10000, 4, 19, 33, 13, 7, 7, 10, 11, -10000, 13, 9, 8, 6, -10000, 11, 15, 10, -10000, 9, -10000, 12, 11, -10000, 13, 10, 23, 6, 0, 9, 13, 17, -10000, 14, 15, -10000, 14, -10000, -10000, 8, -10000, -10000, 14, -10000, 3, 3, 27, 18, -10000, -10000, -10000, 5, -10000, 21, 6, 20, -10000, -10000, 7, 8, -10000, 6, 7, -10000, -10000, 22, 8, 9, 6, -10000, 20, 14, 12, -10000, -10000, 11, 17, 6, 7, 13, -10000, -10000, 5, 6, -10000, 11, 14, 10, -10000, 12, 7, 11, 18, 8, 13, -10000, 4, 5, 11, 8, 7, -10000, 13, 8, 15, 16, 10, -10000, 13, -10000, 6, 3, 10, 3, 3, -10000, -10000, 8, 9, 5, 11, -10000, 14, 12, 7, -10000, -10000, -10000, 3, 4, 7, 6, -10000, 7, 12, 17, 6, 7, -10000, 11, 0, 16, 15, 10, 12, 9, 14, 5, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(puns_het) == len(gold_het)\n",
        "assert len(gold_het) == len(location_het)"
      ],
      "metadata": {
        "id": "B6-7nqa6_-L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Puns of the Day"
      ],
      "metadata": {
        "id": "_BNAAP3vBpFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts_PTD = []\n",
        "labels_PTD = []\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# text = 'My first birthday was great. My 2. was even better.'\n",
        "# sentences = [str(tok) for sent in nlp(text).sents for tok in sent]\n",
        "\n",
        "# opening the CSV file\n",
        "with open(\"/content/drive/My Drive/puns_pos_neg_data.csv\", mode ='r') as file:\n",
        "\n",
        "    # reading the CSV file\n",
        "    csvFile = csv.reader(file)\n",
        "    \n",
        "    # displaying the contents of the CSV file\n",
        "    for line in csvFile:\n",
        "        #print(line)\n",
        "        labels_PTD.append(0 if line[0] == \"-1\" else 1)\n",
        "        # texts_PTD.append([str(tok) for sent in nlp(line[1]).sents for tok in sent])\n",
        "        texts_PTD.append(line[1].split())\n",
        "\n",
        "del texts_PTD[0] # delete the head\n",
        "del labels_PTD[0] # delete the head\n",
        "assert len(texts_PTD) == len(labels_PTD)\n",
        "print(texts_PTD[0])\n",
        "print(texts_PTD[1])"
      ],
      "metadata": {
        "id": "ml4Wp0C_BsvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc275a5-cc96-4d3d-c309-c831cadfe7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'used', 'to', 'be', 'a', 'banker', 'but', 'i', 'lost', 'interest']\n",
            "['i', 'm', 'glad', 'i', 'know', 'sign', 'language', 'it', 's', 'pretty', 'handy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Total Dataset"
      ],
      "metadata": {
        "id": "GXjWFHRnVDHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using spaCy to correct the location index after segmentation."
      ],
      "metadata": {
        "id": "XZW4NWL3EYmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new_puns_hom = []\n",
        "#new_puns_het = []\n",
        "new_location_hom = []\n",
        "new_location_het = []"
      ],
      "metadata": {
        "id": "j_NulTIz8-ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, senten_list in enumerate(puns_hom):\n",
        "    new_sent = [str(tok) for sent in nlp(' '.join(senten_list)).sents for tok in sent]\n",
        "    # new_puns_hom.append(new_sent)\n",
        "    # DO NOT update! Because the dataset package will using the origial sentence to split again!\n",
        "    # If split more than 1 times, the result may be different\n",
        "    if gold_hom[idx] > 0.5:\n",
        "        if new_sent[:location_hom[idx]+1] != senten_list[:location_hom[idx]+1]:\n",
        "            print(senten_list)\n",
        "            print(new_sent)\n",
        "            print(senten_list[location_hom[idx]])\n",
        "            print(location_hom[idx])\n",
        "            count_add = 0\n",
        "            for each_word in senten_list[:location_hom[idx]]:\n",
        "                count_add += len(nlp(each_word))\n",
        "            print(new_sent[count_add])\n",
        "            print(count_add)\n",
        "            new_location_hom.append(count_add)\n",
        "        else:\n",
        "            new_location_hom.append(location_hom[idx])\n",
        "    else:\n",
        "        new_location_hom.append(location_hom[idx]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFufCdq12qiy",
        "outputId": "31e09ed1-573a-421b-bf8a-7c5dc17a9eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don’t', 'let', 'adversity', 'get', 'you', 'down', 'unless', 'you', 'are', 'on', 'your', 'knees']\n",
            "['Do', 'n’t', 'let', 'adversity', 'get', 'you', 'down', 'unless', 'you', 'are', 'on', 'your', 'knees']\n",
            "down\n",
            "5\n",
            "down\n",
            "6\n",
            "['We’ve', 'got', 'children’s', 'church', 'because', 'we', 'know', 'how', 'to', 'kid', 'around']\n",
            "['We', '’ve', 'got', 'children', '’s', 'church', 'because', 'we', 'know', 'how', 'to', 'kid', 'around']\n",
            "kid\n",
            "9\n",
            "kid\n",
            "11\n",
            "['Don’t', 'let', 'your', 'footprints', 'in', 'the', 'sands', 'of', 'time', 'be', 'the', 'mark', 'of', 'a', 'heel']\n",
            "['Do', 'n’t', 'let', 'your', 'footprints', 'in', 'the', 'sands', 'of', 'time', 'be', 'the', 'mark', 'of', 'a', 'heel']\n",
            "heel\n",
            "14\n",
            "heel\n",
            "15\n",
            "['We', 'don’t', 'read', 'palms', ';', 'we', 'wave', 'them', '.']\n",
            "['We', 'do', 'n’t', 'read', 'palms', ';', 'we', 'wave', 'them', '.']\n",
            "palms\n",
            "3\n",
            "palms\n",
            "4\n",
            "['There’s', 'no', 'high', 'like', 'the', 'most', 'high', '.']\n",
            "['There', '’s', 'no', 'high', 'like', 'the', 'most', 'high', '.']\n",
            "high\n",
            "2\n",
            "high\n",
            "3\n",
            "['If', 'you’re', 'looking', 'for', 'a', 'sign', 'from', 'god', ',', 'this', 'is', 'it', '.']\n",
            "['If', 'you', '’re', 'looking', 'for', 'a', 'sign', 'from', 'god', ',', 'this', 'is', 'it', '.']\n",
            "sign\n",
            "5\n",
            "sign\n",
            "6\n",
            "['OLD', 'INTERPRETERS', '(for', 'the', 'deaf)', 'never', 'die', ',', 'they', 'just', 'sign', 'off', '.']\n",
            "['OLD', 'INTERPRETERS', '(', 'for', 'the', 'deaf', ')', 'never', 'die', ',', 'they', 'just', 'sign', 'off', '.']\n",
            "sign\n",
            "10\n",
            "sign\n",
            "12\n",
            "['Just', 'because', 'pews', 'come', 'in', 'rows', 'doesn’t', 'mean', 'we', 'should', 'plant', 'ourselves', 'there']\n",
            "['Just', 'because', 'pews', 'come', 'in', 'rows', 'does', 'n’t', 'mean', 'we', 'should', 'plant', 'ourselves', 'there']\n",
            "plant\n",
            "10\n",
            "plant\n",
            "11\n",
            "['Hell’s', 'the', 'pits']\n",
            "['Hell', '’s', 'the', 'pits']\n",
            "pits\n",
            "2\n",
            "pits\n",
            "3\n",
            "['Satan', 'can’t', 'bring', 'you', 'down', 'any', 'farther', 'than', 'your', 'knees', '.']\n",
            "['Satan', 'ca', 'n’t', 'bring', 'you', 'down', 'any', 'farther', 'than', 'your', 'knees', '.']\n",
            "bring\n",
            "2\n",
            "bring\n",
            "3\n",
            "['Eve', 'taught', 'us', 'it’s', 'okay', 'to', 'take', 'a', 'little', 'ribbing']\n",
            "['Eve', 'taught', 'us', 'it', '’s', 'okay', 'to', 'take', 'a', 'little', 'ribbing']\n",
            "ribbing\n",
            "9\n",
            "ribbing\n",
            "10\n",
            "['OLD', 'SHEET', 'ROCKERS', '(dry', 'wallers)', 'never', 'die', ',', 'they', 'just', 'hang', 'around', '.']\n",
            "['OLD', 'SHEET', 'ROCKERS', '(', 'dry', 'wallers', ')', 'never', 'die', ',', 'they', 'just', 'hang', 'around', '.']\n",
            "hang\n",
            "10\n",
            "hang\n",
            "12\n",
            "['Life’s', 'a', 'ledger', ',', 'are', 'we', 'giving', 'a', 'good', 'accounting', 'of', 'ourselves']\n",
            "['Life', '’s', 'a', 'ledger', ',', 'are', 'we', 'giving', 'a', 'good', 'accounting', 'of', 'ourselves']\n",
            "accounting\n",
            "9\n",
            "accounting\n",
            "10\n",
            "['If', 'you', 'burn', 'the', 'candle', 'on', 'both', 'ends', ',', 'you’re', 'not', 'as', 'bright', 'as', 'you', 'think', '.']\n",
            "['If', 'you', 'burn', 'the', 'candle', 'on', 'both', 'ends', ',', 'you', '’re', 'not', 'as', 'bright', 'as', 'you', 'think', '.']\n",
            "bright\n",
            "12\n",
            "bright\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, senten_list in enumerate(puns_het):\n",
        "    new_sent = [str(tok) for sent in nlp(' '.join(senten_list)).sents for tok in sent]\n",
        "    # new_puns_het.append(new_sent)\n",
        "    # DO NOT update! Because the dataset package will using the origial sentence to split again!\n",
        "    # If split more than 1 times, the result may be different\n",
        "    if gold_het[idx] > 0.5:\n",
        "        if new_sent[:location_het[idx]+1] != senten_list[:location_het[idx]+1]:\n",
        "            print(senten_list)\n",
        "            print(new_sent)\n",
        "            print(senten_list[location_het[idx]])\n",
        "            print(location_het[idx])\n",
        "            count_add = 0\n",
        "            for each_word in senten_list[:location_het[idx]]:\n",
        "                count_add += len(nlp(each_word))\n",
        "            print(new_sent[count_add])\n",
        "            print(count_add)\n",
        "            new_location_het.append(count_add)\n",
        "        else:\n",
        "            new_location_het.append(location_het[idx])\n",
        "    else:\n",
        "        new_location_het.append(location_het[idx])       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrUWJ9Wj3S39",
        "outputId": "604f79b9-f63b-4a7a-e4a3-7b635cc785b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"'\", 'Because', \"'\", 'is', 'a', 'word', 'to', 'the', 'whys', '.']\n",
            "[\"'\", 'Because', \"'\", 'is', 'a', 'word', 'to', 'the', 'why', 's', '.']\n",
            "whys\n",
            "8\n",
            "why\n",
            "8\n",
            "['I', 'have', 'always', 'wanted', 'to', 'hand', 'out', 'carts', 'at', 'Wal', '-', 'Mart', '.', 'I', 'cannot', 'imagine', 'a', 'greeter', 'job', '.']\n",
            "['I', 'have', 'always', 'wanted', 'to', 'hand', 'out', 'carts', 'at', 'Wal', '-', 'Mart', '.', 'I', 'can', 'not', 'imagine', 'a', 'greeter', 'job', '.']\n",
            "greeter\n",
            "17\n",
            "greeter\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assert len(new_puns_hom) == len(gold_hom)\n",
        "assert len(gold_hom) == len(new_location_hom)\n",
        "# assert len(new_puns_het) == len(gold_het)\n",
        "assert len(gold_het) == len(new_location_het)"
      ],
      "metadata": {
        "id": "nrMhTTNv-5fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(gold_hom))\n",
        "print(len(gold_het))\n",
        "print()\n",
        "print(sum(gold_hom))\n",
        "print(sum(gold_het))\n",
        "print()\n",
        "\n",
        "num_pos_task_7 = sum(gold_hom + gold_het)\n",
        "print(num_pos_task_7)\n",
        "num_neg_task_7 = len(gold_hom + gold_het) - num_pos_task_7\n",
        "print(num_neg_task_7)\n",
        "num_delta = num_pos_task_7 - num_neg_task_7\n",
        "print(num_delta)\n",
        "\n",
        "# PTD: from iindex=2423 is neg\n",
        "total_puns = puns_hom + puns_het + texts_PTD[-num_delta:]\n",
        "total_gold = gold_hom + gold_het + labels_PTD[-num_delta:]\n",
        "total_location = new_location_hom + new_location_het + [-10000 for _ in range(num_delta)]\n",
        "# -10000 means has no pun in the sentence\n",
        "assert len(total_puns) == len(total_gold)\n",
        "assert len(total_gold) == len(total_location)\n",
        "assert sum(total_gold) * 2 == len(total_puns)\n",
        "print()\n",
        "\n",
        "print(len(total_puns))\n",
        "print(sum(total_gold))\n",
        "print(sum([1 for i in total_location if i == -10000]))"
      ],
      "metadata": {
        "id": "eUxs1XzalZv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9cd689f-05a8-4d60-e703-5b4960ba8ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2250\n",
            "1780\n",
            "\n",
            "1607\n",
            "1271\n",
            "\n",
            "2878\n",
            "1152\n",
            "1726\n",
            "\n",
            "5756\n",
            "2878\n",
            "2878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hom_pos_index_list = [i for i, x in enumerate(gold_hom) if x == 1]\n",
        "het_pos_index_list = [i for i, x in enumerate(gold_het) if x == 1]\n",
        "print(hom_pos_index_list)\n",
        "print(het_pos_index_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QpHqwFrRKWj",
        "outputId": "996a7ad5-2cc7-4d4c-d695-d285133fdc7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 37, 38, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 54, 57, 59, 60, 61, 62, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 84, 85, 86, 87, 89, 90, 91, 92, 95, 96, 97, 98, 99, 102, 103, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 118, 119, 123, 124, 125, 126, 127, 129, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 155, 156, 158, 159, 160, 161, 162, 163, 164, 166, 168, 169, 171, 172, 173, 174, 176, 177, 178, 180, 182, 184, 185, 187, 188, 190, 192, 194, 195, 196, 197, 198, 200, 201, 202, 203, 205, 206, 207, 208, 213, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 239, 242, 244, 245, 246, 249, 250, 253, 254, 255, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 271, 272, 273, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 287, 288, 291, 292, 293, 295, 297, 299, 300, 302, 303, 304, 307, 309, 312, 314, 315, 316, 317, 318, 319, 321, 322, 323, 324, 326, 327, 328, 329, 333, 337, 338, 340, 341, 343, 344, 345, 347, 348, 349, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 370, 372, 373, 374, 377, 379, 380, 381, 382, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 398, 399, 400, 401, 402, 403, 404, 405, 407, 408, 410, 411, 413, 414, 415, 416, 417, 420, 421, 422, 423, 425, 427, 428, 429, 430, 431, 432, 433, 436, 437, 439, 440, 441, 442, 443, 445, 446, 448, 450, 451, 453, 456, 457, 461, 462, 463, 466, 467, 469, 472, 477, 478, 480, 481, 482, 484, 485, 486, 487, 489, 490, 492, 494, 495, 496, 497, 499, 501, 502, 503, 504, 505, 507, 508, 510, 511, 512, 513, 515, 519, 521, 523, 524, 525, 526, 528, 530, 533, 534, 537, 539, 541, 542, 543, 544, 545, 546, 547, 548, 549, 551, 552, 553, 554, 555, 556, 557, 558, 562, 564, 565, 567, 568, 569, 570, 571, 572, 573, 574, 575, 577, 578, 579, 580, 581, 583, 585, 586, 587, 589, 590, 592, 593, 594, 595, 596, 597, 598, 599, 601, 603, 604, 607, 608, 611, 613, 614, 618, 619, 620, 621, 622, 623, 624, 626, 627, 628, 629, 630, 634, 635, 636, 637, 639, 641, 643, 645, 647, 648, 649, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 665, 666, 667, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 683, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 696, 697, 698, 699, 700, 702, 703, 704, 706, 707, 709, 710, 711, 712, 713, 714, 715, 716, 717, 722, 725, 726, 727, 728, 730, 731, 734, 735, 736, 737, 739, 740, 741, 743, 744, 747, 748, 750, 752, 753, 755, 757, 758, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 773, 774, 777, 778, 779, 782, 784, 786, 787, 788, 789, 790, 791, 792, 795, 798, 799, 801, 803, 804, 805, 806, 807, 809, 810, 811, 813, 814, 816, 818, 819, 820, 824, 827, 828, 832, 833, 834, 835, 837, 840, 842, 843, 844, 846, 847, 850, 851, 852, 853, 854, 855, 856, 857, 859, 860, 861, 862, 863, 865, 867, 869, 870, 871, 872, 875, 876, 877, 880, 881, 883, 884, 885, 887, 888, 889, 890, 894, 896, 897, 899, 900, 901, 902, 905, 906, 907, 909, 910, 911, 912, 913, 914, 915, 916, 917, 919, 921, 922, 923, 924, 925, 927, 928, 933, 936, 937, 938, 939, 940, 941, 942, 944, 945, 947, 948, 953, 954, 955, 956, 957, 958, 959, 960, 961, 963, 965, 967, 968, 969, 970, 971, 972, 973, 975, 976, 977, 978, 980, 982, 983, 984, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 997, 998, 1001, 1003, 1005, 1006, 1007, 1008, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1021, 1022, 1023, 1024, 1026, 1027, 1029, 1030, 1031, 1033, 1034, 1035, 1036, 1038, 1039, 1041, 1042, 1043, 1044, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1059, 1060, 1061, 1062, 1064, 1065, 1066, 1067, 1068, 1070, 1071, 1072, 1073, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1087, 1089, 1090, 1091, 1092, 1099, 1100, 1101, 1102, 1104, 1106, 1108, 1109, 1110, 1111, 1112, 1113, 1115, 1116, 1118, 1119, 1121, 1122, 1123, 1125, 1126, 1127, 1129, 1131, 1132, 1133, 1134, 1135, 1138, 1139, 1140, 1142, 1143, 1146, 1147, 1150, 1152, 1153, 1154, 1155, 1157, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1167, 1170, 1171, 1172, 1173, 1174, 1176, 1177, 1178, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1199, 1200, 1202, 1205, 1208, 1210, 1211, 1212, 1213, 1214, 1216, 1218, 1221, 1223, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1237, 1238, 1240, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1250, 1252, 1253, 1256, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1266, 1267, 1268, 1269, 1270, 1271, 1273, 1274, 1275, 1276, 1277, 1278, 1280, 1281, 1282, 1283, 1285, 1286, 1289, 1290, 1291, 1293, 1294, 1295, 1297, 1299, 1300, 1301, 1303, 1304, 1306, 1307, 1308, 1309, 1310, 1311, 1313, 1314, 1316, 1319, 1320, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1330, 1331, 1333, 1335, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1347, 1348, 1349, 1350, 1351, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1371, 1372, 1373, 1374, 1376, 1377, 1378, 1380, 1381, 1382, 1383, 1384, 1386, 1388, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1399, 1400, 1402, 1403, 1406, 1407, 1409, 1411, 1413, 1415, 1416, 1417, 1418, 1419, 1420, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1443, 1446, 1448, 1449, 1450, 1451, 1452, 1455, 1457, 1458, 1459, 1461, 1462, 1464, 1465, 1468, 1469, 1470, 1472, 1475, 1476, 1478, 1480, 1481, 1482, 1484, 1485, 1486, 1487, 1490, 1491, 1494, 1495, 1496, 1498, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1522, 1525, 1526, 1528, 1530, 1531, 1533, 1534, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1550, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1566, 1568, 1569, 1572, 1573, 1577, 1578, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1595, 1601, 1603, 1604, 1606, 1607, 1608, 1609, 1613, 1614, 1615, 1616, 1617, 1619, 1620, 1621, 1623, 1624, 1625, 1626, 1627, 1628, 1630, 1631, 1634, 1635, 1636, 1637, 1641, 1644, 1645, 1646, 1647, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1662, 1664, 1665, 1669, 1670, 1674, 1675, 1676, 1677, 1679, 1681, 1684, 1685, 1686, 1688, 1689, 1690, 1691, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1706, 1707, 1708, 1710, 1711, 1712, 1713, 1714, 1716, 1718, 1719, 1725, 1726, 1727, 1728, 1729, 1731, 1733, 1734, 1735, 1736, 1738, 1741, 1743, 1746, 1747, 1748, 1749, 1750, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1765, 1766, 1767, 1769, 1770, 1772, 1774, 1777, 1779, 1780, 1781, 1782, 1784, 1785, 1786, 1789, 1790, 1791, 1793, 1794, 1795, 1796, 1797, 1798, 1800, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1811, 1812, 1813, 1814, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1827, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1839, 1840, 1842, 1843, 1845, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1865, 1867, 1869, 1870, 1871, 1872, 1874, 1875, 1876, 1877, 1878, 1879, 1882, 1883, 1884, 1885, 1886, 1887, 1890, 1891, 1893, 1902, 1904, 1905, 1906, 1911, 1912, 1914, 1916, 1917, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1931, 1932, 1933, 1935, 1937, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1959, 1961, 1962, 1963, 1964, 1965, 1967, 1968, 1969, 1970, 1971, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1981, 1982, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1993, 1994, 1995, 1998, 2000, 2001, 2003, 2004, 2005, 2007, 2008, 2009, 2010, 2011, 2013, 2014, 2015, 2016, 2017, 2018, 2020, 2022, 2023, 2024, 2026, 2027, 2028, 2029, 2031, 2032, 2033, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2043, 2044, 2047, 2049, 2050, 2051, 2052, 2054, 2055, 2056, 2057, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2070, 2071, 2072, 2075, 2076, 2077, 2078, 2079, 2080, 2082, 2083, 2084, 2086, 2087, 2088, 2090, 2091, 2092, 2093, 2095, 2096, 2098, 2099, 2100, 2101, 2102, 2103, 2105, 2106, 2107, 2109, 2110, 2112, 2113, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2123, 2124, 2125, 2126, 2128, 2131, 2132, 2133, 2134, 2135, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2146, 2147, 2149, 2151, 2152, 2154, 2156, 2157, 2158, 2159, 2160, 2162, 2163, 2164, 2165, 2167, 2168, 2169, 2172, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2182, 2183, 2184, 2185, 2186, 2190, 2191, 2196, 2199, 2201, 2203, 2206, 2207, 2208, 2209, 2211, 2212, 2213, 2215, 2216, 2219, 2221, 2223, 2226, 2227, 2228, 2230, 2232, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2243, 2244, 2247, 2248, 2249]\n",
            "[0, 1, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 18, 19, 21, 22, 23, 24, 25, 27, 30, 32, 33, 34, 35, 37, 38, 39, 41, 42, 43, 44, 45, 46, 48, 49, 50, 53, 54, 55, 57, 58, 59, 61, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 85, 86, 88, 89, 91, 93, 94, 95, 96, 98, 99, 100, 101, 103, 104, 105, 106, 107, 110, 111, 112, 115, 116, 118, 120, 121, 122, 123, 124, 125, 128, 130, 132, 133, 135, 136, 137, 139, 140, 141, 142, 143, 144, 147, 148, 149, 152, 153, 155, 156, 157, 158, 161, 164, 165, 167, 171, 173, 176, 178, 180, 182, 183, 184, 185, 186, 187, 188, 190, 192, 193, 194, 195, 196, 199, 200, 201, 202, 203, 205, 207, 208, 212, 214, 215, 216, 217, 218, 220, 221, 222, 223, 226, 227, 228, 230, 233, 234, 235, 238, 239, 241, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 256, 257, 258, 259, 260, 262, 263, 264, 266, 267, 269, 271, 275, 276, 277, 281, 282, 283, 284, 286, 287, 288, 290, 291, 292, 293, 295, 297, 298, 299, 300, 301, 302, 304, 305, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 321, 322, 323, 324, 325, 327, 328, 330, 331, 332, 333, 334, 335, 336, 337, 340, 341, 342, 343, 344, 345, 346, 347, 349, 350, 352, 353, 355, 356, 357, 359, 360, 361, 362, 363, 364, 365, 368, 369, 372, 373, 374, 376, 378, 379, 380, 381, 383, 385, 386, 387, 388, 390, 392, 393, 395, 397, 398, 399, 404, 405, 407, 408, 409, 410, 411, 414, 415, 416, 417, 418, 419, 420, 423, 425, 426, 427, 428, 429, 430, 433, 434, 435, 439, 441, 443, 444, 445, 448, 449, 450, 452, 453, 456, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 472, 476, 477, 478, 479, 480, 481, 482, 484, 485, 486, 487, 488, 490, 491, 492, 493, 495, 496, 497, 498, 500, 501, 502, 503, 504, 505, 507, 508, 509, 512, 513, 515, 516, 517, 518, 519, 521, 522, 523, 524, 526, 527, 529, 533, 534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 547, 548, 549, 550, 551, 552, 553, 554, 555, 557, 558, 559, 560, 561, 563, 565, 566, 570, 572, 573, 574, 577, 578, 579, 581, 582, 583, 584, 585, 586, 587, 588, 590, 591, 592, 594, 596, 599, 600, 601, 604, 605, 606, 607, 608, 609, 610, 611, 616, 617, 618, 620, 621, 623, 624, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 637, 638, 639, 642, 643, 644, 645, 646, 648, 650, 651, 652, 653, 654, 655, 656, 660, 661, 662, 664, 665, 666, 667, 668, 669, 670, 671, 673, 674, 675, 676, 677, 680, 682, 683, 684, 685, 688, 689, 690, 691, 692, 693, 696, 697, 698, 699, 700, 702, 703, 704, 705, 707, 708, 709, 710, 713, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 726, 729, 730, 731, 732, 734, 735, 736, 739, 741, 742, 745, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 767, 768, 769, 770, 772, 774, 775, 776, 778, 780, 781, 782, 783, 784, 785, 787, 790, 791, 792, 793, 794, 796, 797, 798, 801, 802, 803, 804, 805, 806, 807, 809, 812, 813, 814, 817, 819, 820, 821, 823, 824, 825, 826, 827, 828, 829, 831, 833, 835, 836, 839, 841, 842, 843, 846, 847, 848, 850, 851, 853, 854, 856, 858, 860, 862, 863, 865, 866, 868, 869, 870, 871, 873, 874, 875, 876, 879, 880, 881, 882, 884, 885, 886, 887, 888, 889, 890, 891, 892, 894, 896, 897, 899, 900, 901, 902, 903, 904, 906, 907, 908, 909, 910, 911, 912, 913, 914, 919, 920, 921, 922, 923, 925, 927, 928, 929, 932, 933, 934, 936, 938, 939, 941, 942, 943, 944, 947, 948, 949, 950, 951, 952, 953, 954, 955, 957, 963, 968, 970, 972, 973, 976, 977, 979, 980, 981, 982, 983, 984, 985, 986, 988, 989, 990, 991, 992, 994, 996, 997, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1009, 1010, 1011, 1012, 1013, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1023, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1041, 1042, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1060, 1061, 1062, 1064, 1065, 1066, 1069, 1070, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1081, 1083, 1084, 1085, 1087, 1088, 1089, 1090, 1093, 1094, 1095, 1096, 1097, 1098, 1100, 1102, 1103, 1104, 1105, 1106, 1108, 1109, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1121, 1122, 1125, 1126, 1127, 1128, 1129, 1131, 1132, 1133, 1135, 1136, 1137, 1139, 1140, 1141, 1142, 1143, 1146, 1147, 1149, 1151, 1154, 1155, 1156, 1158, 1159, 1163, 1166, 1167, 1170, 1172, 1173, 1174, 1175, 1176, 1179, 1180, 1182, 1183, 1184, 1185, 1189, 1192, 1193, 1194, 1196, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1208, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1220, 1221, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1236, 1237, 1238, 1240, 1241, 1242, 1243, 1246, 1247, 1248, 1249, 1250, 1253, 1254, 1255, 1258, 1259, 1260, 1261, 1263, 1264, 1265, 1267, 1268, 1270, 1271, 1272, 1274, 1275, 1277, 1279, 1281, 1282, 1283, 1284, 1285, 1286, 1288, 1289, 1290, 1291, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1307, 1309, 1310, 1311, 1313, 1314, 1315, 1316, 1318, 1321, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1334, 1335, 1336, 1338, 1339, 1341, 1344, 1345, 1346, 1347, 1348, 1350, 1352, 1353, 1355, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1366, 1367, 1368, 1369, 1370, 1371, 1377, 1378, 1379, 1380, 1381, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1400, 1401, 1402, 1404, 1405, 1406, 1411, 1412, 1413, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1424, 1426, 1427, 1428, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1440, 1441, 1442, 1443, 1445, 1446, 1448, 1449, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1460, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1493, 1494, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1513, 1514, 1517, 1518, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1536, 1537, 1538, 1540, 1541, 1543, 1544, 1545, 1546, 1548, 1552, 1553, 1554, 1555, 1556, 1557, 1559, 1560, 1562, 1565, 1568, 1570, 1571, 1574, 1575, 1577, 1578, 1579, 1580, 1581, 1582, 1585, 1586, 1587, 1588, 1589, 1593, 1594, 1596, 1597, 1598, 1599, 1601, 1603, 1604, 1605, 1607, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1618, 1621, 1623, 1624, 1625, 1626, 1628, 1629, 1630, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1641, 1642, 1643, 1644, 1646, 1647, 1648, 1650, 1652, 1653, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1664, 1665, 1667, 1670, 1673, 1675, 1676, 1677, 1678, 1682, 1684, 1685, 1686, 1689, 1690, 1692, 1693, 1696, 1697, 1698, 1699, 1701, 1702, 1703, 1706, 1707, 1708, 1709, 1710, 1713, 1714, 1716, 1717, 1718, 1720, 1721, 1722, 1723, 1724, 1725, 1727, 1728, 1729, 1730, 1731, 1733, 1734, 1735, 1736, 1737, 1739, 1741, 1742, 1743, 1744, 1745, 1748, 1749, 1750, 1751, 1753, 1754, 1755, 1759, 1760, 1761, 1762, 1764, 1765, 1766, 1767, 1768, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Wu_Palmer_similarity(sentence, word_list):\n",
        "    \"\"\"\n",
        "    Wu-Palmer similarity (wup_similarity):\n",
        "    Range: [0, 1]\n",
        "    Minimum: 0 (no relationship)\n",
        "    Maximum: 1 (identical synsets)\n",
        "    \"\"\"\n",
        "    # No self-loop\n",
        "    # Example sentence\n",
        "    # sentence = \"The cat chased the dog\"\n",
        "\n",
        "    # Calculate semantic relatedness using Wu-Palmer similarity\n",
        "    # word1 = 'dog'\n",
        "    # word2 = 'cat'\n",
        "    # synset1 = lesk(nltk.word_tokenize(sentence), word1, 'n')  # Disambiguate word1\n",
        "    # synset2 = lesk(nltk.word_tokenize(sentence), word2, 'n')  # Disambiguate word2\n",
        "\n",
        "    edge_weight = torch.rand((len(word_list) * (len(word_list) - 1),))\n",
        "    # Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)\n",
        "    nltk_word_tokenize_sentence = nltk.word_tokenize(sentence)\n",
        "    synset_list = []\n",
        "\n",
        "    for word in word_list:\n",
        "        synset_list.append(lesk(nltk_word_tokenize_sentence, word, 'n'))\n",
        "\n",
        "    for i in range(len(word_list)):\n",
        "        for j in range(i + 1, len(word_list)):\n",
        "            if isinstance(synset_list[0], Synset) and isinstance(synset_list[1], Synset):\n",
        "                similarity = synset_list[0].wup_similarity(synset_list[1])\n",
        "                # similarity = synset1.wup_similarity(synset2)\n",
        "                # The same as synset2.wup_similarity(synset1)\n",
        "                # print(\"Similarity between\", word1, \"and\", word2, \":\", similarity)\n",
        "                edge_weight[j * (len(word_list) - 1) + i + (-1 if i > j else 0)] = similarity\n",
        "                edge_weight[i * (len(word_list) - 1) + j + (-1 if j > i else 0)] = similarity\n",
        "            #else:\n",
        "                # print(\"Cannot compute similarity. One or both words are not in WordNet.\")\n",
        "                # random samples from a uniform distribution over [0, 1).\n",
        "\n",
        "    return edge_weight"
      ],
      "metadata": {
        "id": "cpw4CIAMrxip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(texts, labels, tokenizer, max_length, total_location):\n",
        "    # Each element of the texts is word list\n",
        "    assert len(texts) == len(labels)\n",
        "    dataset = []\n",
        "\n",
        "    for data_index in tqdm(range(len(labels))):\n",
        "        text, label, location = texts[data_index], labels[data_index], total_location[data_index]\n",
        "        # Define the input text\n",
        "        # text = ['We’ve', 'got', 'children’s', 'church', 'because', 'we', 'know', 'how', 'to', 'kid', 'around']\n",
        "        corresponding_location = -10000\n",
        "        id_range = [] # The word belong to which ids [start, end)\n",
        "\n",
        "        # Process the text with the spaCy NLP pipeline\n",
        "        words = [str(tok) for sent in (nlp(' '.join(text))).sents for tok in sent]\n",
        "        #words = text\n",
        "        #print(words)\n",
        "\n",
        "        corresponding_index = []\n",
        "        # consider which token (in tokens[], base on index) to be the corresponding word\n",
        "\n",
        "        current_index = 0\n",
        "        start = False\n",
        "        idx = 0\n",
        "        #print(words)\n",
        "        for word_idx, x in enumerate(words):\n",
        "            #print(x)\n",
        "            subwords = tokenizer.tokenize((' ' if start else '') + x)\n",
        "            start = True\n",
        "            #print(subwords)\n",
        "            for i in range(1, len(subwords)):\n",
        "                subwords[i] = subwords[i][2:] # delete the '##'\n",
        "\n",
        "            # subwords_len = np.zeros((len(subwords),), dtype=float)\n",
        "            subwords_len = torch.zeros((len(subwords),))\n",
        "            for count_index, each_token in enumerate(subwords):\n",
        "                subwords_len[count_index] = len(each_token)\n",
        "            # longest_index = np.argmax(subwords_len, axis=0)\n",
        "            longest_index = int(torch.argmax(subwords_len, dim=0))\n",
        "\n",
        "            #print(longest_index)\n",
        "            if 1 + current_index + longest_index < max_length - 1:\n",
        "                # Cannot longer than max_length\n",
        "                # because the input_ids has a start ID and end ID\n",
        "                # which needs to +1 and -1 to match the index\n",
        "                corresponding_index.append(current_index + longest_index)\n",
        "                id_range.append([current_index, current_index + len(subwords)])\n",
        "                if word_idx == location:\n",
        "                    corresponding_location = current_index + longest_index\n",
        "\n",
        "                current_index += len(subwords)\n",
        "                idx += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        if label > 0.5 and corresponding_location < 0:\n",
        "            print(\"Do not get the corresponding_location!\")\n",
        "\n",
        "        original_idx = idx\n",
        "        if text[:idx] != words[:idx]:\n",
        "            for each_word in text[:idx]:\n",
        "                original_idx -= 1\n",
        "                sub_sent = nlp(' '.join(text[:original_idx]))\n",
        "                if [str(tok) for sent in sub_sent.sents for tok in sent] == words[:idx]:\n",
        "                    break\n",
        "            else:\n",
        "                print(\"Something Wrong!\")\n",
        "\n",
        "        # TODO: Some bugs in here, if the cut word is the last word, it maybe wrong\n",
        "        \n",
        "        doc = nlp(' '.join(text[:original_idx]))\n",
        "        assert [str(tok) for sent in doc.sents for tok in sent] == words[:idx]\n",
        "        graph = nx.DiGraph()\n",
        "        for token in doc:\n",
        "            graph.add_node(token.i, word=token.text)\n",
        "            graph.add_edge(token.head.i, token.i, relation=token.dep_)\n",
        "\n",
        "        #for node in graph.nodes(data=True):\n",
        "        #    print(f\"Node: {node}\")\n",
        "\n",
        "        #print(corresponding_index)\n",
        "        #print(words[:idx])\n",
        "        #print(id_range)\n",
        "        corresponding_index = torch.tensor(corresponding_index, dtype=torch.long)\n",
        "        #print(corresponding_index)\n",
        "        assert len(corresponding_index) == len(words[:idx])\n",
        "        assert len(corresponding_index) == len(id_range)\n",
        "\n",
        "        sentence = ' '.join(words) # be processed in tokenizer\n",
        "        #tokens = tokenizer.tokenize(sentence)\n",
        "        #print(tokens)\n",
        "        \n",
        "        tokenized = tokenizer(sentence, return_tensors=\"pt\", max_length=max_length, return_token_type_ids=False, truncation=True, padding=\"max_length\")\n",
        "        #print(tokenized)\n",
        "\n",
        "        num_nodes = len(corresponding_index)\n",
        "        source_nodes = [i for j in range(num_nodes) for i in range(num_nodes) if i != j]\n",
        "        target_nodes = [j for j in range(num_nodes) for i in range(num_nodes) if i != j]\n",
        "        # Complete directed graph, no self-loop\n",
        "\n",
        "        # Add the root, which is self-loop, let the temporary index is -1, -1, and it is the last element\n",
        "        source_nodes.append(-1)\n",
        "        target_nodes.append(-1)\n",
        "\n",
        "        source_nodes = torch.tensor(source_nodes, dtype=torch.long)\n",
        "        target_nodes = torch.tensor(target_nodes, dtype=torch.long)\n",
        "        edge_index = torch.stack([source_nodes, target_nodes], dim=0)\n",
        "        #print(edge_index)\n",
        "        # print(edge_index.shape[1]) # the number of edges\n",
        "\n",
        "        # edge_weight = torch.rand(edge_index.shape[1])  # Replace this with the actual edge weights of your adjacency matrix\n",
        "        edge_weight = Wu_Palmer_similarity(sentence, words[:idx]) # numpy array\n",
        "        # edge_weight = np.append(edge_weight, [10.0], axis=0) # add the ROOT value\n",
        "        edge_weight = torch.cat((edge_weight, torch.tensor([10.0])), dim=-1)\n",
        "        # edge_weight = torch.tensor(edge_weight, dtype=torch.float32) # the type must dtype=torch.float32\n",
        "        assert edge_index.shape[1] == len(edge_weight)\n",
        "\n",
        "        has_ROOT = False\n",
        "        # Add dependency parsing tree\n",
        "        for edge in graph.edges(data=True):\n",
        "            #print(f\"Edge: {edge}\")\n",
        "            source_index = edge[0]\n",
        "            target_index = edge[1]\n",
        "            if source_index == target_index:\n",
        "                # ROOT\n",
        "                source_nodes[-1] = source_index\n",
        "                target_nodes[-1] = target_index\n",
        "                edge_index[0, len(source_nodes) - 1] = source_index\n",
        "                edge_index[1, len(target_nodes) - 1] = target_index\n",
        "                edge_weight[len(edge_weight) - 1] = 20.0 # may change this number\n",
        "                # print(edge_index[:, len(edge_weight) - 1])\n",
        "                has_ROOT = True\n",
        "            else:\n",
        "                # not ROOT\n",
        "                edge_weight[target_index * (num_nodes - 1) + source_index + (-1 if source_index > target_index else 0)] += 0.8\n",
        "                edge_weight[target_index * (num_nodes - 1) + source_index + (-1 if source_index > target_index else 0)] *= 10\n",
        "                # may change this number\n",
        "                # print(edge_index[:, target_index * (num_nodes - 1) + source_index + (-1 if source_index > target_index else 0)])\n",
        "        if not has_ROOT:\n",
        "            print(\"No ROOT, something wrong!\")\n",
        "\n",
        "        y = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        #tokenized[\"input_ids\"][0] == tokenized[\"input_ids\"].flatten()\n",
        "        #print(corresponding_index)\n",
        "        if len(tokenized[\"input_ids\"][0]) != max_length:\n",
        "            print(len(tokenized[\"input_ids\"][0]))\n",
        "            print(\"The length is wrong!\")\n",
        "        for each_corre_idx, each_corre in enumerate(corresponding_index):\n",
        "            assert id_range[each_corre_idx][0] <= each_corre < id_range[each_corre_idx][1]\n",
        "\n",
        "        data = Data(data_identification=torch.tensor(data_index, dtype=torch.long), # The ID of the data\n",
        "                    input_ids=tokenized[\"input_ids\"][0],\n",
        "                    attention_mask=tokenized[\"attention_mask\"][0],\n",
        "                    corresponding=corresponding_index,\n",
        "                    sentence_len=torch.tensor(len(corresponding_index), dtype=torch.long), # original length\n",
        "                    edge_index=edge_index,\n",
        "                    edge_weight=edge_weight,\n",
        "                    location=torch.tensor(location, dtype=torch.long), # pun word index in original sentence\n",
        "                    corresponding_location=torch.tensor(corresponding_location, dtype=torch.long), # pun word index in ids (start with 0), need to +1\n",
        "                    id_range=torch.tensor(id_range, dtype=torch.long),\n",
        "                    y=y,\n",
        "                    num_nodes=torch.tensor(num_nodes, dtype=torch.long)\n",
        "                    )\n",
        "        # DO NOT corresponding_index=corresponding_index!\n",
        "        # If the end of the name of parameters of Data() has '_index',\n",
        "        # the package will consider this is index list, it will be automatic update the value list:\n",
        "        # Example: [1,2,5], [2,3,9], [3,6,1] -> [1,2,5,8,9,15,19,22,17]\n",
        "        # Not: [1,2,5], [2,3,9], [3,6,1] -> [1,2,5,2,3,9,3,6,1]\n",
        "        # If set num_nodes as parameters of Data(), it will be sum automatically (is a number finally, not a list)\n",
        "        dataset.append(data)\n",
        "        '''\n",
        "        print('tokenized[\"input_ids\"][0]')\n",
        "        print(tokenized[\"input_ids\"][0])\n",
        "        print('tokenized[\"attention_mask\"][0]')\n",
        "        print(tokenized[\"attention_mask\"][0])\n",
        "        print('corresponding_index')\n",
        "        print(corresponding_index)\n",
        "        print('len(corresponding_index)')\n",
        "        print(len(corresponding_index))\n",
        "        print('location')\n",
        "        print(location)\n",
        "        print('corresponding_location')\n",
        "        print(corresponding_location)\n",
        "        print('id_range')\n",
        "        print(id_range)\n",
        "        print('y')\n",
        "        print(y)\n",
        "        '''\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "rhOhbZMsLm7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom GNN and BiLSTM layers\n",
        "class GNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # input_dim is number of features of each node\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch):\n",
        "        x = self.conv1(x, edge_index, edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight)\n",
        "\n",
        "        # Global Pooling (stack different aggregations)\n",
        "        hidden = torch.cat([global_mean_pool(x, batch),\n",
        "                            global_max_pool(x, batch)], dim=1)\n",
        "        return x, hidden\n",
        "\n",
        "\n",
        "class BERT_GNN_MTL_Classifier(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_dim, num_lstm_layers, gnn_hidden_dim, gnn_output_dim, max_length, batch_size):\n",
        "        super().__init__()\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.gnn_output_dim = gnn_output_dim\n",
        "        self.bert_config = BertConfig.from_pretrained(bert_model_name)\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name, config=self.bert_config)\n",
        "        # Embedding size of BERT is 768\n",
        "        self.gnn = GNN(self.bert_config.hidden_size, gnn_hidden_dim, gnn_output_dim)\n",
        "        #self.bilstm = BiLSTM(self.bert_config.hidden_size, hidden_dim, num_lstm_layers)\n",
        "        #self.bilstm = BiLSTM(gnn_output_dim, hidden_dim, num_lstm_layers)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=gnn_output_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_lstm_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            #dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 1) # 2 is bidirectional LSTM, has 2, 1 is the output\n",
        "        # self.fc = nn.Linear(hidden_dim * 2, num_classes) # multi-class classfication\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "        self.classifier = nn.Linear(hidden_dim * (2 if self.bilstm.bidirectional else 1), 1) # sigmoid\n",
        "\n",
        "    def forward(self, data, token_index):\n",
        "        # token_index[i] should be start with 0 and step is 1, because the GNN use only one id per word\n",
        "        input_ids, attention_mask = data.input_ids, data.attention_mask\n",
        "        corresponding_index, sentence_len = data.corresponding, data.sentence_len\n",
        "        edge_index, edge_weight, num_nodes, batch = data.edge_index, data.edge_weight, data.num_nodes, data.batch\n",
        "        #print(sentence_len)\n",
        "        assert sum(sentence_len) == len(corresponding_index)\n",
        "        actual_batch_size = len(input_ids) // self.max_length\n",
        "\n",
        "        first_cut = 0\n",
        "        corres = [] # the corresponding index of each sentence\n",
        "        # print(corresponding_index)\n",
        "        # print(sentence_len)\n",
        "        for each_sent_len_i in range(len(sentence_len) - 1):\n",
        "            corres.append(corresponding_index[first_cut:first_cut+sentence_len[each_sent_len_i]])\n",
        "            first_cut += sentence_len[each_sent_len_i]\n",
        "        corres.append(corresponding_index[first_cut:first_cut+sentence_len[-1]])\n",
        "        # print(corres)\n",
        "        assert len(corres) == actual_batch_size\n",
        "\n",
        "        input_ids = torch.reshape(input_ids, (actual_batch_size, self.max_length))\n",
        "        attention_mask = torch.reshape(attention_mask, (actual_batch_size, self.max_length))\n",
        "        # print(\"input_ids.shape:\", input_ids.shape) # torch.Size([self.batch_size, self.max_length])\n",
        "\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = bert_output.last_hidden_state\n",
        "        # print(\"hidden_states.shape:\", hidden_states.shape) # torch.Size([self.batch_size, self.max_length, 768])\n",
        "\n",
        "        gnn_input = torch.zeros((len(corresponding_index), hidden_states.shape[-1])) # not include the start and end ID\n",
        "        each_batch_index = 0\n",
        "        start_index = 0\n",
        "        each_counter = 0\n",
        "        # print(corresponding_index)\n",
        "        for each_corr_i in corresponding_index:\n",
        "            # each_corr_i + 1 because the input_ids has a start ID in the front,\n",
        "            # which needs to add 1 to match the index\n",
        "            gnn_input[start_index] = hidden_states[each_batch_index, each_corr_i + 1]\n",
        "            start_index += 1\n",
        "            each_counter += 1\n",
        "            if each_counter == sentence_len[each_batch_index]:\n",
        "                each_batch_index += 1\n",
        "                each_counter = 0\n",
        "        each_counter = 0\n",
        "        assert each_batch_index == actual_batch_size\n",
        "        assert start_index == len(corresponding_index)\n",
        "        \n",
        "        gnn_output, gnn_hidden = self.gnn(x=gnn_input, edge_index=edge_index, edge_weight=edge_weight, batch=batch)\n",
        "\n",
        "        bilstm_input = torch.zeros((actual_batch_size, self.max_length, self.gnn_output_dim))\n",
        "        each_batch_index = 0\n",
        "        start_index = 0\n",
        "        # Similar to padding 0\n",
        "        for each_sen_len in sentence_len:\n",
        "            for each_word_i in range(each_sen_len):\n",
        "                bilstm_input[each_batch_index, each_word_i] = gnn_output[start_index]\n",
        "                start_index += 1\n",
        "            each_batch_index += 1\n",
        "\n",
        "        each_batch_index = 0\n",
        "        start_index = 0\n",
        "\n",
        "        lstm_output, (hidden, _) = self.bilstm(bilstm_input)\n",
        "        '''\n",
        "        Anthor way is using only last hidden state of the LSTM cell:\n",
        "\n",
        "        Using lstm_output[:, -1] selects the last hidden state of the LSTM cell\n",
        "        for each sequence in the batch. The reason we use this approach in the\n",
        "        example provided is that the last hidden state is often a good\n",
        "        representation of the entire sequence in many sequence-to-sequence\n",
        "        models, especially for classification tasks.\n",
        "\n",
        "        When we use lstm_output[:, -1], we're selecting the hidden states of the\n",
        "        LSTM cells at the last time step (i.e., the last token in the input sequence)\n",
        "        for each sequence in the batch. This can be a good representation of the\n",
        "        entire sequence for classification tasks since it captures information\n",
        "        from both the forward and backward passes of the sequence.\n",
        "        '''\n",
        "        pooled_output = torch.mean(lstm_output, 1)\n",
        "\n",
        "        if self.bilstm.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "            # hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=-1)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]\n",
        "\n",
        "        # Get the hidden state of the word at the specified index\n",
        "        assert lstm_output.size(0) == len(input_ids)\n",
        "        assert actual_batch_size == lstm_output.size(0)\n",
        "        assert lstm_output.size(2) == self.bilstm.hidden_size * (2 if self.bilstm.bidirectional else 1)\n",
        "\n",
        "        focused_word_hidden = torch.zeros((lstm_output.size(0), lstm_output.size(2)))\n",
        "        for sentence_index in range(lstm_output.size(0)):\n",
        "            # token_index[i] should be start with 0 and step is 1, because the GNN use only one id per word\n",
        "            # If is the pun, token_index[i] should be the same as data.location\n",
        "            # corresponding index (need to be +1) of the pun word: int(corres[sentence_index][token_index[sentence_index]])\n",
        "            # print(int(corres[sentence_index][token_index[sentence_index]]))\n",
        "            focused_word_hidden[sentence_index] = lstm_output[sentence_index, token_index[sentence_index], :]\n",
        "        classification_output = self.classifier(focused_word_hidden)\n",
        "\n",
        "        return self.fc(pooled_output), classification_output\n",
        "        "
      ],
      "metadata": {
        "id": "KkZjYosGLkpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **sigmoid**\n",
        "# If using sigmoid, we should use this accuracy function\n",
        "def calculate_accuracy(output, target):\n",
        "    assert output.shape == target.shape\n",
        "    threshold = 0.5\n",
        "    predictions = (output > threshold).float()\n",
        "    correct = (predictions == target).sum().item()\n",
        "    total = target.numel()\n",
        "    return correct / total\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn_1, loss_fn_2, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy_1 = 0.0\n",
        "    total_accuracy_2 = 0.0\n",
        "    for batch in tqdm(data_loader):\n",
        "        batch = batch.to(device)\n",
        "        labels = batch.y.unsqueeze(1).float().to(device)\n",
        "\n",
        "        token_index = torch.clone(batch.location).to(device)\n",
        "\n",
        "        batch_att = torch.reshape(batch.attention_mask, (len(labels), len(batch.attention_mask) // len(labels)))\n",
        "\n",
        "        for sentence_index in range(len(labels)):\n",
        "            if token_index[sentence_index] < 0:\n",
        "                # No pun word\n",
        "                assert labels[sentence_index][0] < 0.5\n",
        "                token_index[sentence_index] = torch.randint(0, int(sum(batch_att[sentence_index])) - 2, (1,))\n",
        "                # TODO: Should be modified to sentence_len\n",
        "        # print(token_index)\n",
        "        # print(batch.id_range)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits_1, logits_2 = model(batch, token_index)\n",
        "        loss_1 = loss_fn_1(logits_1, labels)\n",
        "        loss_2 = loss_fn_2(logits_2, labels)\n",
        "        joint_loss = loss_1 + loss_2\n",
        "        joint_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += joint_loss.item()\n",
        "        sigmoid_1 = torch.sigmoid(logits_1.view(-1)).unsqueeze(1)\n",
        "        sigmoid_2 = torch.sigmoid(logits_2.view(-1)).unsqueeze(1)\n",
        "        total_accuracy_1 += calculate_accuracy(sigmoid_1, labels)\n",
        "        total_accuracy_2 += calculate_accuracy(sigmoid_2, labels)\n",
        "        #print(f\"Train Loss: {total_loss:.4f}, Train Accuracy: {total_accuracy:.4f}\")\n",
        "\n",
        "    return total_loss / len(data_loader), total_accuracy_1 / len(data_loader), total_accuracy_2 / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_epoch(model, data_loader, loss_fn_1, loss_fn_2, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy_1 = 0.0\n",
        "    total_accuracy_2 = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader):\n",
        "            batch = batch.to(device)\n",
        "            labels = batch.y.unsqueeze(1).float().to(device)\n",
        "\n",
        "            token_index = torch.clone(batch.location).to(device)\n",
        "\n",
        "            batch_att = torch.reshape(batch.attention_mask, (len(labels), len(batch.attention_mask) // len(labels)))\n",
        "\n",
        "            for sentence_index in range(len(labels)):\n",
        "                if token_index[sentence_index] < 0:\n",
        "                    # No pun word\n",
        "                    assert labels[sentence_index][0] < 0.5\n",
        "                    token_index[sentence_index] = torch.randint(0, int(sum(batch_att[sentence_index])) - 2, (1,))\n",
        "                    # TODO: Should be modified to sentence_len\n",
        "                    # random select one location\n",
        "\n",
        "            logits_1, logits_2 = model(batch, token_index)\n",
        "            loss_1 = loss_fn_1(logits_1, labels)\n",
        "            loss_2 = loss_fn_2(logits_2, labels)\n",
        "            joint_loss = loss_1 + loss_2\n",
        "\n",
        "            total_loss += joint_loss.item()\n",
        "            sigmoid_1 = torch.sigmoid(logits_1.view(-1)).unsqueeze(1)\n",
        "            sigmoid_2 = torch.sigmoid(logits_2.view(-1)).unsqueeze(1)\n",
        "            total_accuracy_1 += calculate_accuracy(sigmoid_1, labels)\n",
        "            total_accuracy_2 += calculate_accuracy(sigmoid_2, labels)\n",
        "            #print(f\"Validation Loss: {total_loss:.4f}, Validation Accuracy: {total_accuracy:.4f}\")\n",
        "\n",
        "    return total_loss / len(data_loader), total_accuracy_1 / len(data_loader), total_accuracy_2 / len(data_loader)"
      ],
      "metadata": {
        "id": "0G6CPm3s26gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have your data as lists: train_texts, train_labels, val_texts, and val_labels\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "max_length = 80  # Adjust the maximum length based on your dataset\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "cJWjCNGkEvh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "2c78df9d1b3c436b9655ea0d0ceab84d",
            "ed1e0257bfcc44a18b98d8e348f63b29",
            "729c1d4dcc994bfe8540087ea9e6f9f2",
            "9dd79698eb6645aba7b6d691fdcc23fc",
            "39436ee4e4a14249a56a05ebb1854b57",
            "36861ba77ca74b9cb3d7884f7a2eea0d",
            "6f91cd3fbe364b0aa4952a48d021aac7",
            "c4a81572d1d34e81b6c274fce509f019",
            "46e2e077b238409ea3441fbbf27b7f61",
            "05e79eb9b9c644c8b4e0e3cb1951a022",
            "57a7133414a0490a9e4efd8013f9ce4d",
            "271e1e4a4d3b4bfdb605320e87003e63",
            "98bafa726c0d483aac6c141eb31302b1",
            "49b6963c345d49449cb620295b0963a9",
            "6ac53e64146a4e7ca92dfdb7a1531205",
            "d59496466c1f404496aae9c2eca778b5",
            "21ca4841cafb408f8e6f094722ff3e80",
            "282dfa72472743c38106d120803e3718",
            "f6b0bd3f2dd64725943003d086b96ce4",
            "f69c5f09ab45403685645e9df5e65558",
            "787aabefd6174e288c0ea40fa182e6cd",
            "2d97573887994e09b18b83a67ce93a75",
            "399aa888f5e143b38b930ae6c8460689",
            "371d1d618c84460c85c920ff86391494",
            "3810148fca8c4c13b9bc1218659400d4",
            "45e7e1aac20b4d2dabc987d402c94c80",
            "2a1ea1f814a94fd8a61b164a3da646d0",
            "1ab116b075d148ab8791bb5aaa128c93",
            "c5b7714a55fe4924aeab088b41e1c87f",
            "d6ed0fcb6e964d9aa1e837e23e00756f",
            "f8483061b0f84c7e95883870df27e405",
            "338f9e78fa2c46fc8b88ac5d0f1f9751",
            "735c3763ec514e49857ed75bbdb9f6f8"
          ]
        },
        "outputId": "82e84603-b130-476e-e0b3-a830c4104e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c78df9d1b3c436b9655ea0d0ceab84d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "271e1e4a4d3b4bfdb605320e87003e63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "399aa888f5e143b38b930ae6c8460689"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "dataset = create_dataset(texts=total_puns, labels=total_gold,\n",
        "                         tokenizer=tokenizer, max_length=max_length,\n",
        "                         total_location=total_location)\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/dataset.obj', 'wb')\n",
        "pickle.dump(dataset, fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/dataset.obj', 'rb')\n",
        "dataset = pickle.load(fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    dataset, [len(total_gold)-int(0.2*len(total_gold)), int(0.2*len(total_gold))])\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/train_dataset.obj', 'wb')\n",
        "pickle.dump(train_dataset, fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/train_dataset.obj', 'rb')\n",
        "train_dataset = pickle.load(fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/val_dataset.obj', 'wb')\n",
        "pickle.dump(val_dataset, fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/val_dataset.obj', 'rb')\n",
        "val_dataset = pickle.load(fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "# Create DataLoaders for each set with a batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGCa6FWIG58M",
        "outputId": "fc09ac47-97d3-4ba9-96bf-bf082878ee86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5756/5756 [03:58<00:00, 24.14it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileObj = open('/content/drive/My Drive/dataset.obj', 'rb')\n",
        "dataset = pickle.load(fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/train_dataset.obj', 'rb')\n",
        "train_dataset = pickle.load(fileObj)\n",
        "fileObj.close()\n",
        "\n",
        "fileObj = open('/content/drive/My Drive/val_dataset.obj', 'rb')\n",
        "val_dataset = pickle.load(fileObj)\n",
        "fileObj.close()"
      ],
      "metadata": {
        "id": "eNzHBhSfWqcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the custom BERT model\n",
        "# num_classes = len(set(total_gold))  # Assuming labels are integers starting from 0\n",
        "num_classes = 2\n",
        "hidden_dim = 128\n",
        "num_lstm_layers = 2\n",
        "gnn_hidden_dim = 512\n",
        "gnn_output_dim = 256\n",
        "learning_rate = 2e-5\n",
        "\n",
        "\n",
        "# Set up two different loss function\n",
        "loss_fn_1 = nn.BCEWithLogitsLoss()\n",
        "loss_fn_2 = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "lYXz4RIzE4hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "model = BERT_GNN_MTL_Classifier(num_classes, hidden_dim, num_lstm_layers,\n",
        "                                gnn_hidden_dim, gnn_output_dim, max_length, batch_size).to(device)\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "MNoooAClivfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8d28ca0a313e4effb5f3543a60d90303",
            "2f06f208fb274167a43d40208281b0f5",
            "8987ddc177f042c2bf69a3a406c06cb7",
            "51be03976f464de084ddc4a5c981cd60",
            "af9ba67d55d24c779251fc9f04d24631",
            "ac2271b4f51247d0a6cf2c2ae555cc82",
            "b77a58e297d14d4fbd5e74e649f2ef55",
            "9e572f7b5ebc468fb83f02b5fb60fcae",
            "9244c07a848145f99dbed23cbfc91861",
            "fee5a993f35d4a42989ed75c0ba9955e",
            "d7efd2626bde4a7193b0416f9d6c94e5"
          ]
        },
        "outputId": "a67d0589-d863-4c86-fdd8-4433a8acf4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d28ca0a313e4effb5f3543a60d90303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT_GNN_MTL_Classifier(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (gnn): GNN(\n",
            "    (conv1): GCNConv(768, 512)\n",
            "    (conv2): GCNConv(512, 256)\n",
            "  )\n",
            "  (bilstm): LSTM(256, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (classifier): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_loss, train_acc_1, train_acc_2 = train_epoch(model, train_loader, loss_fn_1, loss_fn_2, optimizer, device)\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc_1:.4f}, Training Word Accuracy: {train_acc_2:.4f}\")\n",
        "    torch.save(model, model_path) # save the entire model, including the architecture\n",
        "    val_loss, val_acc_1, val_acc_2 = eval_epoch(model, val_loader, loss_fn_1, loss_fn_2, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc_1:.4f}, Validation Word Accuracy: {val_acc_2:.4f}\")\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "Ihn7ANfMFPEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b562b9-4e4c-4713-b5cf-3019e83ea8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 144/144 [1:08:24<00:00, 28.50s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.0905, Training Accuracy: 0.7912, Training Word Accuracy: 0.8748\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [05:11<00:00,  8.64s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.8388, Validation Accuracy: 0.9209, Validation Word Accuracy: 0.9217\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 144/144 [1:07:16<00:00, 28.03s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.7024, Training Accuracy: 0.9463, Training Word Accuracy: 0.9455\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [05:13<00:00,  8.71s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.5926, Validation Accuracy: 0.9469, Validation Word Accuracy: 0.9469\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 144/144 [1:06:08<00:00, 27.56s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.4844, Training Accuracy: 0.9668, Training Word Accuracy: 0.9670\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [05:10<00:00,  8.61s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 0.5063, Validation Accuracy: 0.9391, Validation Word Accuracy: 0.9435\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 6/144 [02:49<1:03:45, 27.72s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the the saved file\n",
        "loaded_model = torch.load(model_path).to(device)\n",
        "\n",
        "# Set the model to evaluation mode if you plan to use it for inference\n",
        "loaded_model.eval()\n",
        "\n",
        "location_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    y_true_hom = []\n",
        "    y_pred_hom = []\n",
        "    y_true_het = []\n",
        "    y_pred_het = []\n",
        "    y_true_neg = []\n",
        "    y_pred_neg = []\n",
        "\n",
        "    location_hom_correct = 0\n",
        "    location_het_correct = 0\n",
        "\n",
        "    num_correct, num_samples, num_accuracy = 0, 0, 0\n",
        "    for ele in tqdm(location_loader):\n",
        "        # print(ele.attention_mask)\n",
        "        attention_mask = ele.attention_mask.reshape(1, max_length).to(device)\n",
        "        #labels = ele['label'].unsqueeze(1).float().to(device)\n",
        "        label = ele.y.to(device)\n",
        "        # corresponding_token_index = torch.clone(ele.corresponding_location).to(device)\n",
        "        token_index = torch.clone(ele.location).to(device) # After GNN, the same as original, start with 0\n",
        "        # id_range = ele.id_range.to(device)\n",
        "        data_identification = ele.data_identification.to(device)\n",
        "\n",
        "        #print(input_ids)\n",
        "        #print(attention_mask)\n",
        "        #print(label[0])\n",
        "        #print(int(token_index[0]))\n",
        "        #print(id_range)\n",
        "        #print(ele)\n",
        "\n",
        "        if float(label[0]) < 0.5:\n",
        "            y_true_neg.append(0)\n",
        "            # This has not pun, do not count\n",
        "            logits_1, _ = loaded_model(ele, torch.tensor(0).reshape(1,))\n",
        "            y_pred_neg.append(0 if float(torch.sigmoid(logits_1.view(-1))) < 0.5 else 1)\n",
        "            continue\n",
        "        \n",
        "        scores_list = torch.zeros((int(sum(attention_mask[0])) - 2,))\n",
        "        for used_token_index in range(int(sum(attention_mask[0])) - 2): # TODO: Should be modified to sentence_len\n",
        "            # After GNN, the index is same as original, start with 0\n",
        "            logits_1, logits_2 = loaded_model(ele, torch.tensor(used_token_index).reshape(1,))\n",
        "            scores_list[used_token_index] = float(torch.sigmoid(logits_2.view(-1))) # Get each ID scores\n",
        "\n",
        "        # TODO: If the matched token ID is belonged to the pun word, it is also correct.\n",
        "        if int(torch.argmax(scores_list, dim=0)) == int(token_index[0]):\n",
        "            num_correct += 1\n",
        "\n",
        "            if int(data_identification[0]) < 2250:\n",
        "                location_hom_correct += 1\n",
        "            elif 2250 <= int(data_identification[0]) < 2250 + 1780:\n",
        "                location_het_correct += 1\n",
        "            else:\n",
        "                print(\"Something Wrong\")\n",
        "        \n",
        "        if int(data_identification[0]) < 2250:\n",
        "            y_true_hom.append(1)\n",
        "            y_pred_hom.append(0 if float(torch.sigmoid(logits_1.view(-1))) < 0.5 else 1)\n",
        "        elif 2250 <= int(data_identification[0]) < 2250 + 1780:\n",
        "            y_true_het.append(1)\n",
        "            y_pred_het.append(0 if float(torch.sigmoid(logits_1.view(-1))) < 0.5 else 1)\n",
        "        else:\n",
        "            print(\"Something Wrong\")\n",
        "\n",
        "        num_samples += 1\n",
        "        #if num_samples == 300:\n",
        "        #    break\n",
        "\n",
        "    num_accuracy = num_correct / num_samples\n",
        "    print(f'Location Accuracy: {num_accuracy:.4f}')\n",
        "    print(f'Homographic Location Accuracy: {(location_hom_correct / len(y_true_hom)):.4f}')\n",
        "    print(f'Heterographic Location Accuracy: {(location_het_correct / len(y_true_het)):.4f}')\n",
        "    assert len(y_true_hom) + len(y_true_het) == num_samples\n",
        "    print()\n",
        "    print(classification_report(y_true_hom+y_true_neg[:len(y_true_hom)], y_pred_hom+y_pred_neg[:len(y_true_hom)], target_names=['No Puns', 'Homographic']))\n",
        "    print(confusion_matrix(y_true_hom+y_true_neg[:len(y_true_hom)], y_pred_hom+y_pred_neg[:len(y_true_hom)]))\n",
        "    print()\n",
        "    print(classification_report(y_true_het+y_true_neg[:len(y_true_het)], y_pred_het+y_pred_neg[:len(y_true_het)], target_names=['No Puns', 'Heterographic']))\n",
        "    print(confusion_matrix(y_true_het+y_true_neg[:len(y_true_het)], y_pred_het+y_pred_neg[:len(y_true_het)]))"
      ],
      "metadata": {
        "id": "L0G90vuAriUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09bffbe5-9219-4164-b53b-c3d1b0225cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1151/1151 [53:58<00:00,  2.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Location Accuracy: 0.0564\n",
            "Homographic Location Accuracy: 0.0583\n",
            "Heterographic Location Accuracy: 0.0543\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     No Puns       0.95      0.94      0.94       309\n",
            " Homographic       0.94      0.95      0.94       309\n",
            "\n",
            "    accuracy                           0.94       618\n",
            "   macro avg       0.94      0.94      0.94       618\n",
            "weighted avg       0.94      0.94      0.94       618\n",
            "\n",
            "[[290  19]\n",
            " [ 16 293]]\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      No Puns       0.94      0.93      0.94       258\n",
            "Heterographic       0.93      0.94      0.94       258\n",
            "\n",
            "     accuracy                           0.94       516\n",
            "    macro avg       0.94      0.94      0.94       516\n",
            " weighted avg       0.94      0.94      0.94       516\n",
            "\n",
            "[[241  17]\n",
            " [ 15 243]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}