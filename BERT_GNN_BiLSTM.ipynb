{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120753,"status":"ok","timestamp":1682908045725,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"},"user_tz":240},"id":"G5bO4vaxqnW1","outputId":"9175694b-d11c-481b-a256-451371c79d7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","Collecting torch-scatter\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-scatter\n","Successfully installed torch-scatter-2.1.1+pt20cu118\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","Collecting torch-sparse\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n","Installing collected packages: torch-sparse\n","Successfully installed torch-sparse-0.6.17+pt20cu118\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","Collecting torch-cluster\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.22.4)\n","Installing collected packages: torch-cluster\n","Successfully installed torch-cluster-1.6.1+pt20cu118\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","Collecting torch-spline-conv\n","  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (884 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.9/884.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-spline-conv\n","Successfully installed torch-spline-conv-1.2.2+pt20cu118\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n","Building wheels for collected packages: torch-geometric\n","  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910476 sha256=0bf0dada800543ee8d351308e9dad89873d06b067a18738fbe00e37732278ade\n","  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n","Successfully built torch-geometric\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.3.1\n","2023-05-01 02:27:10.784350: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-01 02:27:12.570037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!pip install transformers\n","!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-2.0.0+cu118.html\n","# The same as Torch version and CUDA version (torch.__version__ is 2.0.0+cu118)\n","!pip install torch-geometric\n","!python3 -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39077,"status":"ok","timestamp":1682908098661,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"},"user_tz":240},"id":"5oS1ij6urEV9","outputId":"1afdadc8-8059-4427-a2e9-ec79e5a755e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.0+cu118\n","Mounted at /content/drive\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","print(torch.__version__)\n","from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\n","from torch_geometric.data import Data, DataLoader\n","import numpy as np\n","from transformers import BertModel, BertTokenizer, BertConfig\n","import csv\n","import spacy\n","import networkx as nx\n","#from gensim.models import Word2Vec\n","from tqdm import tqdm\n","import xml.etree.ElementTree as ET\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cv6IguateR9"},"outputs":[],"source":["# Custom GNN and BiLSTM layers\n","class GNN(torch.nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        # input_dim is number of features of each node\n","        super(GNN, self).__init__()\n","        self.conv1 = GCNConv(input_dim, hidden_dim)\n","        self.conv2 = GCNConv(hidden_dim, output_dim)\n","\n","    def forward(self, x, edge_index, edge_weight, batch):\n","        x = self.conv1(x, edge_index, edge_weight)\n","        x = F.relu(x)\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index, edge_weight)\n","        return x\n","\n","class BiLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        output, _ = self.lstm(x)\n","        return output\n","\n","\n","class BERT_GNN_Classifier(nn.Module):\n","    def __init__(self, num_classes, hidden_dim, num_lstm_layers, gnn_hidden_dim, gnn_output_dim, max_length, batch_size):\n","        super().__init__()\n","        self.max_length = max_length\n","        self.batch_size = batch_size\n","        self.gnn_output_dim = gnn_output_dim\n","        self.bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n","        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", config=self.bert_config)\n","        # Embedding size of BERT is 768\n","        self.gnn = GNN(self.bert_config.hidden_size, gnn_hidden_dim, gnn_output_dim)\n","        #self.bilstm = BiLSTM(self.bert_config.hidden_size, hidden_dim, num_lstm_layers)\n","        self.bilstm = BiLSTM(gnn_output_dim, hidden_dim, num_lstm_layers)\n","        self.fc = nn.Linear(hidden_dim * 2, 1) # 2 is bidirectional LSTM, has 2, 1 is the output\n","        # self.fc = nn.Linear(hidden_dim * 2, num_classes) # multi-class classfication\n","        # self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, data):\n","        input_ids, attention_mask = data.input_ids, data.attention_mask\n","        corresponding_index, sentence_len = data.corresponding, data.sentence_len\n","        edge_index, edge_weight, num_nodes, batch = data.edge_index, data.edge_weight, data.num_nodes, data.batch\n","        #print(sentence_len)\n","        assert sum(sentence_len) == len(corresponding_index)\n","        actual_batch_size = len(input_ids) // self.max_length\n","\n","        input_ids = torch.reshape(input_ids, (actual_batch_size, self.max_length))\n","        attention_mask = torch.reshape(attention_mask, (actual_batch_size, self.max_length))\n","        # print(\"input_ids.shape:\", input_ids.shape) # torch.Size([self.batch_size, self.max_length])\n","\n","        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_states = bert_output.last_hidden_state\n","        # print(\"hidden_states.shape:\", hidden_states.shape) # torch.Size([self.batch_size, self.max_length, 768])\n","\n","        gnn_input = torch.zeros((len(corresponding_index), hidden_states.shape[-1]))\n","        each_batch_index = 0\n","        start_index = 0\n","        each_counter = 0\n","        # print(corresponding_index)\n","        for each_corr_i in corresponding_index:\n","            # each_corr_i + 1 because the input_ids has a start ID in the front,\n","            # which needs to add 1 to match the index\n","            gnn_input[start_index] = hidden_states[each_batch_index, each_corr_i + 1]\n","            start_index += 1\n","            each_counter += 1\n","            if each_counter == sentence_len[each_batch_index]:\n","                each_batch_index += 1\n","                each_counter = 0\n","        each_counter = 0\n","        assert each_batch_index == actual_batch_size\n","        assert start_index == len(corresponding_index)\n","        \n","        gnn_output = self.gnn(x=gnn_input, edge_index=edge_index, edge_weight=edge_weight, batch=batch)\n","\n","        bilstm_input = torch.zeros((actual_batch_size, self.max_length, self.gnn_output_dim))\n","        each_batch_index = 0\n","        start_index = 0\n","        # Similar to padding\n","        for each_sen_len in sentence_len:\n","            for each_word_i in range(each_sen_len):\n","                bilstm_input[each_batch_index, each_word_i] = gnn_output[start_index]\n","                start_index += 1\n","            each_batch_index += 1\n","\n","        each_batch_index = 0\n","        start_index = 0\n","\n","        bilstm_output = self.bilstm(bilstm_input)\n","        '''\n","        Anthor way is using only last hidden state of the LSTM cell:\n","\n","        Using lstm_output[:, -1] selects the last hidden state of the LSTM cell\n","        for each sequence in the batch. The reason we use this approach in the\n","        example provided is that the last hidden state is often a good\n","        representation of the entire sequence in many sequence-to-sequence\n","        models, especially for classification tasks.\n","\n","        When we use lstm_output[:, -1], we're selecting the hidden states of the\n","        LSTM cells at the last time step (i.e., the last token in the input sequence)\n","        for each sequence in the batch. This can be a good representation of the\n","        entire sequence for classification tasks since it captures information\n","        from both the forward and backward passes of the sequence.\n","        '''\n","        pooled_output = torch.mean(bilstm_output, 1)\n","        logits = self.fc(pooled_output)\n","        # return self.sigmoid(logits)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CX_4yGLOu7Gg"},"outputs":[],"source":["def create_dataset(texts, labels, tokenizer, max_length):\n","    assert len(texts) == len(labels)\n","    dataset = []\n","\n","    # Load the English language model\n","    nlp = spacy.load(\"en_core_web_sm\")\n","\n","    for data_index in tqdm(range(len(labels))):\n","        text, label = texts[data_index], labels[data_index]\n","        # Define the input text\n","        # text = \"What will you purchase me for my birthday in July?\"\n","\n","        # Process the text with the spaCy NLP pipeline\n","        doc = nlp(text)\n","\n","        # Create a directed graph\n","        graph = nx.DiGraph()\n","\n","        # Add nodes and edges based on the dependency relations\n","        for token in doc:\n","            graph.add_node(token.i, word=token.text)\n","            graph.add_edge(token.head.i, token.i, relation=token.dep_)\n","\n","        # Print the graph's nodes and edges\n","\n","        # print(len(graph.nodes(data=True)))\n","        words = []\n","        for node in graph.nodes(data=True):\n","            #print(f\"Node: {node}\")\n","            #print(node)\n","            words.append(node)\n","        words = sorted(words, key=lambda x: x[0])\n","        words = [x[1]['word'] for x in words]\n","        #print(words)\n","\n","        sentence = ' '.join(words)\n","\n","        corresponding_index = []\n","        # consider which token (in tokens[], base on index) to be the corresponding word\n","\n","        current_index = 0\n","        start = False\n","        idx = 0\n","        #print(words)\n","        for x in words:\n","            #print(x)\n","            subwords = tokenizer.tokenize((' ' if start else '') + x)\n","            start = True\n","            #print(subwords)\n","            subwords_new = [subwords[0]]\n","            for i in range(1, len(subwords)):\n","                subwords_new.append(subwords[i][2:]) # delete the '##'\n","            #print(subwords_new)\n","            subwords_new_len = np.zeros((len(subwords_new),), dtype=float)\n","            for count_index, each_token in enumerate(subwords_new):\n","                subwords_new_len[count_index] = len(each_token)\n","            longest_index = np.argmax(subwords_new_len, axis=0)\n","            #print(longest_index)\n","            if 1 + current_index + longest_index < max_length - 1:\n","                # Cannot longer than max_length\n","                # because the input_ids has a start ID and end ID\n","                # which needs to +1 and -1 to match the index\n","                corresponding_index.append(current_index + longest_index)\n","                current_index += len(subwords)\n","                idx += 1\n","            else:\n","                # Do Again! (the sentence is shorter)\n","                doc = nlp(' '.join(words[:idx]))\n","                graph = nx.DiGraph()\n","                for token in doc:\n","                    graph.add_node(token.i, word=token.text)\n","                    graph.add_edge(token.head.i, token.i, relation=token.dep_)\n","                break\n","\n","        #print(corresponding_index)\n","        corresponding_index = torch.tensor(corresponding_index, dtype=torch.long)\n","        #print(corresponding_index)\n","        if len(corresponding_index) != len(words[:idx]):\n","            print(\"Words number error!\")\n","\n","        tokens = tokenizer.tokenize(sentence)\n","        #print(tokens)\n","        \n","        tokenized = tokenizer(sentence, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n","        #print(tokenized)\n","\n","        num_nodes = len(corresponding_index)\n","        source_nodes = [i for j in range(num_nodes) for i in range(num_nodes) if i != j]\n","        target_nodes = [j for j in range(num_nodes) for i in range(num_nodes) if i != j]\n","        # Complete directed graph, no self-loop\n","\n","        # Add the root, which is self-loop, let the temporary index is -1, -1, and it is the last element\n","        source_nodes.append(-1)\n","        target_nodes.append(-1)\n","\n","        source_nodes = torch.tensor(source_nodes, dtype=torch.long)\n","        target_nodes = torch.tensor(target_nodes, dtype=torch.long)\n","        edge_index = torch.stack([source_nodes, target_nodes], dim=0)\n","        #print(edge_index)\n","        # print(edge_index.shape[1]) # the number of edges\n","\n","        # TODO: add edge weigth\n","        # edge_weight = torch.rand(edge_index.shape[1])  # Replace this with the actual edge weights of your adjacency matrix\n","        edge_weight = torch.ones(edge_index.shape[1])\n","\n","        has_ROOT = False\n","        # Add dependency parsing tree\n","        for edge in graph.edges(data=True):\n","            # print(f\"Edge: {edge}\")\n","            #print(edge)\n","            source_index = edge[0]\n","            target_index = edge[1]\n","            if source_index == target_index:\n","                # ROOT\n","                source_nodes[-1] = source_index\n","                target_nodes[-1] = target_index\n","                edge_index[0, len(source_nodes) - 1] = source_index\n","                edge_index[1, len(target_nodes) - 1] = target_index\n","                edge_weight[len(edge_weight) - 1] = 10.0 # TODO: may change this number\n","                # print(edge_index[:, len(edge_weight) - 1])\n","                has_ROOT = True\n","            else:\n","                # not ROOT\n","                edge_weight[target_index * (num_nodes - 1) + source_index + (-1 if source_index > target_index else 0)] *= 10.0\n","                # TODO: may change this number\n","                # print(edge_index[:, target_index * (num_nodes - 1) + source_index + (-1 if source_index > target_index else 0)])\n","        if not has_ROOT:\n","            print(\"No ROOT, something wrong!\")\n","\n","        y = torch.tensor(label, dtype=torch.long)\n","\n","        #tokenized[\"input_ids\"][0] == tokenized[\"input_ids\"].flatten()\n","        #print(corresponding_index)\n","        if len(tokenized[\"input_ids\"][0]) != max_length:\n","            print(len(tokenized[\"input_ids\"][0]))\n","            print(\"The length is wrong!\")\n","        data = Data(input_ids=tokenized[\"input_ids\"][0], attention_mask=tokenized[\"attention_mask\"][0],\n","                    corresponding=corresponding_index, sentence_len=torch.tensor(len(corresponding_index), dtype=torch.long),\n","                    edge_index=edge_index, edge_weight=edge_weight, y=y, num_nodes=torch.tensor(num_nodes, dtype=torch.long))\n","        # DO NOT corresponding_index=corresponding_index!\n","        # If the end of the name of parameters of Data() has '_index',\n","        # the package will consider this is index list, it will be automatic update the value list:\n","        # Example: [1,2,5], [2,3,9], [3,6,1] -> [1,2,5,8,9,15,19,22,17]\n","        # Not: [1,2,5], [2,3,9], [3,6,1] -> [1,2,5,2,3,9,3,6,1]\n","        # If set num_nodes as parameters of Data(), it will be sum automatically (is a number finally, not a list)\n","        dataset.append(data)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUzILI8_vKNq"},"outputs":[],"source":["# **sigmoid**\n","# If using sigmoid, we should use this accuracy function\n","def calculate_accuracy(output, target):\n","    threshold = 0.5\n","    predictions = (output > threshold).float()\n","    correct = (predictions == target).sum().item()\n","    total = target.numel()\n","    return correct / total\n","\n","def train_epoch(model, data_loader, loss_fn, optimizer, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_accuracy = 0.0\n","    for batch in tqdm(data_loader):\n","        batch = batch.to(device)\n","        labels = batch.y.unsqueeze(1).float().to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(batch)\n","        loss = loss_fn(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        sigmoid = torch.sigmoid(logits.view(-1)).unsqueeze(1)\n","        total_accuracy += calculate_accuracy(sigmoid, labels)\n","        #print(f\"Train Loss: {total_loss:.4f}, Train Accuracy: {total_accuracy:.4f}\")\n","\n","    return total_loss / len(data_loader), total_accuracy / len(data_loader)\n","\n","\n","def eval_epoch(model, data_loader, loss_fn, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_accuracy = 0.0\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader):\n","            batch = batch.to(device)\n","            labels = batch.y.unsqueeze(1).float().to(device)\n","\n","            logits = model(batch)\n","            loss = loss_fn(logits, labels)\n","\n","            total_loss += loss.item()\n","            sigmoid = torch.sigmoid(logits.view(-1)).unsqueeze(1)\n","            total_accuracy += calculate_accuracy(sigmoid, labels)\n","            #print(f\"Validation Loss: {total_loss:.4f}, Validation Accuracy: {total_accuracy:.4f}\")\n","\n","    return total_loss / len(data_loader), total_accuracy / len(data_loader)"]},{"cell_type":"code","source":["# Assuming you have your data as lists: train_texts, train_labels, val_texts, and val_labels\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","max_length = 50  # Adjust the maximum length based on your dataset\n","batch_size = 32\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["4d7493199ef44caebde3a17c6f9020a0","f836493c43214b96b74866b5f7cd6398","1da9681bdd4643b9b4f948c22cacb835","e0d86976dac246f0b8772e0d92e6dc8a","2645e93391cd4cdea171f58bbc8ee491","70d6f9196d274118ad4efd20aa6b50a8","f747ea9342824ac2a4dd930df2fd0134","0e0f8c88d13e4c849c0f967fc7f321c3","93b1ac6fdfca4f4f9d1bbffc2bbfda3a","25a2c54c96b545f88fa5030497636f35","1770b70ae18741058a3d1dda11022e8b","06aabf3ec7964d70aa654ee98459abad","06b66cc5c63f404881224bf742016940","2c4f6250dae0436ba869bc69b4fbc01e","7ca9d3d97f9f430e87743ca5b5a70335","3b2b5c2977104f04a2436a1d988d03e5","4fb68a267b5d445ba16e977b037f09c3","6b928b28f8d045c6b49c1a30d82f24ae","9b447cea33fa471fb7555f1c65ea9903","0deef85ac6cc42f89c6ec91886401895","8836a0373aa64c8e97fdef01aa2ad5d3","55475b7453cc4b449bab8265e59f174e","b38d33f9f81945f7bd998f9da9a64203","1bdc8a4473f4479fbb2fca4fe32ee3ce","3feb2a7dff6143d29c6a1724aa364afd","5834a9609ab64fac872f0e671d64ed90","45ed757cdb3e44b9ba059e457b71f0c7","475c434195ed47b1a5f64c20534ccb50","9372524ce8e544aeb3540c60ae1e6709","cc24b9e862e849b98a9bb59715934cc0","d62bb059e8734cee951a74d663cd8650","98f7b3168d014bbc889750b0297c9dcc","d12df694e4f64923ad54d15a47ece828"]},"id":"rt1WT46lJxax","executionInfo":{"status":"ok","timestamp":1682908343731,"user_tz":240,"elapsed":2295,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"bd5bb2bb-64cf-4a64-cf15-ff3e941a1304"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d7493199ef44caebde3a17c6f9020a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06aabf3ec7964d70aa654ee98459abad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38d33f9f81945f7bd998f9da9a64203"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96879,"status":"ok","timestamp":1682908676539,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"},"user_tz":240},"id":"4BZqbmicdlal","outputId":"52bb3df1-64a0-4d4c-dbce-a64bd58934a8"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4826/4826 [01:34<00:00, 51.02it/s]\n"]}],"source":["texts_PTD = []\n","labels_PTD = []\n","\n","# opening the CSV file\n","with open(\"/content/drive/My Drive/puns_pos_neg_data.csv\", mode ='r') as file:\n","\n","    # reading the CSV file\n","    csvFile = csv.reader(file)\n","    \n","    # displaying the contents of the CSV file\n","    for line in csvFile:\n","        #print(line)\n","        labels_PTD.append(0 if line[0] == \"-1\" else 1)\n","        texts_PTD.append(line[1])\n","\n","del texts_PTD[0] # delete the head\n","del labels_PTD[0] # delete the head\n","\n","# Create data loaders\n","dataset = create_dataset(texts=texts_PTD, labels=labels_PTD,\n","                         tokenizer=tokenizer, max_length=max_length)\n","\n","# Split the dataset into training and validation sets\n","train_dataset, val_dataset = torch.utils.data.random_split(\n","    dataset, [len(labels_PTD)-int(0.2*len(labels_PTD)), int(0.2*len(labels_PTD))])\n","\n","# Create DataLoaders for each set with a batch size\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","source":["# Initialize the custom BERT model\n","num_classes = len(set(labels_PTD))  # Assuming labels are integers starting from 0\n","hidden_dim = 128\n","num_lstm_layers = 2\n","gnn_hidden_dim = 512\n","gnn_output_dim = 256\n","learning_rate = 2e-5\n","model_path = \"/content/drive/My Drive/my_GNN_PT_model.pt\"  # Choose your desired path and filename\n","\n","\n","# Set up the loss function and optimizer\n","# loss_fn = nn.CrossEntropyLoss() # use by softmax\n","loss_fn = nn.BCEWithLogitsLoss()"],"metadata":{"id":"Iiboi5KJKm-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":587,"referenced_widgets":["88863592db9a4182b033310a64c1ec5b","2b1d750b536042c89f2d1e1ea6f994b1","b0a57da38a394bb7af4545b9405b20ca","3a1daac9d5aa406daa7d3280c5b27737","13a1e7df126240008bedce8b2c4fe5d0","7fbfdd4e438745109deafb2f4a62cac7","a3c3916a997b4a80ac21c6da10a251aa","cbf5ceafd597464f8220d6e7cdf7e771","52f0f31fe19249c4a7c5fe2a0bf13e6a","47b4fa6f33ec4d038ac233f1def99c6e","c01ac64a14044e1c919c21b5d3303f1e"]},"id":"mfTqf5_7vO4S","outputId":"3f4edd18-de60-4ad2-e6f2-1373b53783fd","executionInfo":{"status":"ok","timestamp":1682889979559,"user_tz":240,"elapsed":7630262,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88863592db9a4182b033310a64c1ec5b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 121/121 [23:48<00:00, 11.81s/it]\n","100%|██████████| 31/31 [01:50<00:00,  3.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.5895, Validation Loss: 0.4559, Train Accuracy: 0.7923, Validation Accuracy: 0.8827\n","\n","Epoch 2/5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 121/121 [23:35<00:00, 11.70s/it]\n","100%|██████████| 31/31 [01:50<00:00,  3.56s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.3710, Validation Loss: 0.3703, Train Accuracy: 0.9179, Validation Accuracy: 0.8833\n","\n","Epoch 3/5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 121/121 [23:39<00:00, 11.73s/it]\n","100%|██████████| 31/31 [01:50<00:00,  3.55s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.2456, Validation Loss: 0.2680, Train Accuracy: 0.9434, Validation Accuracy: 0.9179\n","\n","Epoch 4/5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 121/121 [23:26<00:00, 11.63s/it]\n","100%|██████████| 31/31 [01:50<00:00,  3.55s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.1511, Validation Loss: 0.2409, Train Accuracy: 0.9674, Validation Accuracy: 0.9143\n","\n","Epoch 5/5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 121/121 [23:25<00:00, 11.62s/it]\n","100%|██████████| 31/31 [01:48<00:00,  3.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0937, Validation Loss: 0.2463, Train Accuracy: 0.9773, Validation Accuracy: 0.9210\n","\n","Training complete.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["model = BERT_GNN_Classifier(num_classes, hidden_dim, num_lstm_layers,\n","                            gnn_hidden_dim, gnn_output_dim, max_length, batch_size).to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Train the model\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train_loss, train_acc = train_epoch(model, train_loader, loss_fn, optimizer, device)\n","    torch.save(model, model_path) # save the entire model, including the architecture\n","    val_loss, val_acc = eval_epoch(model, val_loader, loss_fn, device)\n","    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\\n\")\n","print(\"Training complete.\")"]},{"cell_type":"code","source":["# model_path = \"/content/drive/My Drive/my_GNN_PT_model.pt\"\n","loaded_model = torch.load(model_path)"],"metadata":{"id":"Ar6xxRV-IatV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ub_-Tfv8Ue8w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683059268682,"user_tz":240,"elapsed":2683,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"02ad7b75-0a65-4714-9b7f-9e55f5d28824"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-05-02 20:27:45--  https://alt.qcri.org/semeval2017/task7/data/uploads/semeval2017_task7.tar.xz\n","Resolving alt.qcri.org (alt.qcri.org)... 80.76.166.231\n","Connecting to alt.qcri.org (alt.qcri.org)|80.76.166.231|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 748424 (731K) [application/x-xz]\n","Saving to: ‘semeval2017_task7.tar.xz’\n","\n","semeval2017_task7.t 100%[===================>] 730.88K   823KB/s    in 0.9s    \n","\n","2023-05-02 20:27:48 (823 KB/s) - ‘semeval2017_task7.tar.xz’ saved [748424/748424]\n","\n","\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34msemeval2017_task7\u001b[0m/  semeval2017_task7.tar.xz\n"]}],"source":["!wget https://alt.qcri.org/semeval2017/task7/data/uploads/semeval2017_task7.tar.xz\n","!tar -xf semeval2017_task7.tar.xz\n","#!tar -xvf semeval2017_task7.tar.xz\n","#%cd semeval2017_task7/\n","#%cd ..\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7dNmNx6Uo6-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682889980506,"user_tz":240,"elapsed":4,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"f6adec47-7931-48cc-c099-08edbd7b8a0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'het_1': {'het_1_1': \"'\", 'het_1_2': \"'\", 'het_1_3': 'I', 'het_1_4': \"'\", 'het_1_5': 'm', 'het_1_6': 'halfway', 'het_1_7': 'up', 'het_1_8': 'a', 'het_1_9': 'mountain', 'het_1_10': ',', 'het_1_11': \"'\", 'het_1_12': \"'\", 'het_1_13': 'Tom', 'het_1_14': 'alleged', 'het_1_15': '.'}}\n"]}],"source":["f = 'semeval2017_task7/data/test/subtask1-heterographic-test.xml'\n","\n","mytree = ET.parse(f)\n","myroot = mytree.getroot()\n","\n","puns = []\n","for item in myroot.findall('./text'):\n","  dict1 = {}\n","  dict1[item.attrib['id']] = {}\n","  for child in item:\n","    idd = child.attrib['id']\n","    dict1[item.attrib['id']][idd] = child.text\n","  puns.append(dict1)\n","\n","print(puns[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6Jba8edUrMg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682890091907,"user_tz":240,"elapsed":139,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"bb488a9d-da94-49bc-84d4-b458f5bcc7c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["gold = []\n","with open('semeval2017_task7/data/test/subtask1-heterographic-test.gold', 'r') as fin:\n","  for row in fin:\n","    gold.append(int(row.strip().split('\\t')[1]))\n","print(gold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TzrzG4KUr5v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682890837927,"user_tz":240,"elapsed":125,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"f3e19562-0545-4d6e-b07e-70d73cd0d91e"},"outputs":[{"output_type":"stream","name":"stdout","text":["1780\n","1780\n","' ' I ' m halfway up a mountain , ' ' Tom alleged .\n"]}],"source":["subtask1_heterographic = []\n","for i in puns:\n","    for pun in i.values():\n","        #poss = [x for x in pun]\n","        sentence = ' '.join([pun[x].replace(u'\\xa0', '_') for x in pun])\n","        # print(sentence)\n","        subtask1_heterographic.append(sentence)\n","\n","print(len(gold))\n","print(len(subtask1_heterographic))\n","print(subtask1_heterographic[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5aWOhMGYrV7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682891061248,"user_tz":240,"elapsed":221539,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"97d53ee6-923f-41d8-860c-3202fdea8f85"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1780/1780 [00:18<00:00, 98.09it/s]\n","/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n","  warnings.warn(out)\n","100%|██████████| 56/56 [03:22<00:00,  3.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.5280, Test Accuracy: 0.8258\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["test_dataset = create_dataset(texts=subtask1_heterographic, labels=gold,\n","                         tokenizer=tokenizer, max_length=max_length)\n","\n","# Create DataLoaders for each set with a batch size\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","test_loss, test_acc = eval_epoch(loaded_model, test_loader, loss_fn, device)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","source":["f = 'semeval2017_task7/data/test/subtask1-homographic-test.xml'\n","\n","mytree = ET.parse(f)\n","myroot = mytree.getroot()\n","\n","puns = []\n","for item in myroot.findall('./text'):\n","  dict1 = {}\n","  dict1[item.attrib['id']] = {}\n","  for child in item:\n","    idd = child.attrib['id']\n","    dict1[item.attrib['id']][idd] = child.text\n","  puns.append(dict1)\n","\n","print(puns[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Swpg0x-JHjOS","executionInfo":{"status":"ok","timestamp":1682908212856,"user_tz":240,"elapsed":575,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"042d37cd-7520-4903-a4ce-6360af6d031d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'hom_1': {'hom_1_1': 'They', 'hom_1_2': 'hid', 'hom_1_3': 'from', 'hom_1_4': 'the', 'hom_1_5': 'gunman', 'hom_1_6': 'in', 'hom_1_7': 'a', 'hom_1_8': 'sauna', 'hom_1_9': 'where', 'hom_1_10': 'they', 'hom_1_11': 'could', 'hom_1_12': 'sweat', 'hom_1_13': 'it', 'hom_1_14': 'out', 'hom_1_15': '.'}}\n"]}]},{"cell_type":"code","source":["gold = []\n","with open('semeval2017_task7/data/test/subtask1-homographic-test.gold', 'r') as fin:\n","  for row in fin:\n","    gold.append(int(row.strip().split('\\t')[1]))\n","print(gold)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhEcU6fIHlPY","executionInfo":{"status":"ok","timestamp":1682908215343,"user_tz":240,"elapsed":252,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"f720f603-62f8-4da2-9184-52f9edc45053"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1]\n"]}]},{"cell_type":"code","source":["subtask1_homographic = []\n","for i in puns:\n","    for pun in i.values():\n","        #poss = [x for x in pun]\n","        sentence = ' '.join([pun[x].replace(u'\\xa0', '_') for x in pun])\n","        # print(sentence)\n","        subtask1_homographic.append(sentence)\n","\n","print(len(gold))\n","print(len(subtask1_homographic))\n","print(subtask1_homographic[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwT8KT5bH8as","executionInfo":{"status":"ok","timestamp":1682908217132,"user_tz":240,"elapsed":276,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"717a8605-5ee5-4aa0-846b-5818c71b782a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2250\n","2250\n","They hid from the gunman in a sauna where they could sweat it out .\n"]}]},{"cell_type":"code","source":["test_dataset = create_dataset(texts=subtask1_homographic, labels=gold,\n","                         tokenizer=tokenizer, max_length=max_length)\n","\n","# Create DataLoaders for each set with a batch size\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","test_loss, test_acc = eval_epoch(loaded_model, test_loader, loss_fn, device)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLVhJt0xHpfi","executionInfo":{"status":"ok","timestamp":1682909137530,"user_tz":240,"elapsed":454333,"user":{"displayName":"Liangyu Li","userId":"13176302498120902496"}},"outputId":"983f49d4-0b4e-46f0-cd58-55a99ade38a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2250/2250 [00:35<00:00, 63.73it/s]\n","100%|██████████| 71/71 [06:57<00:00,  5.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.4749, Test Accuracy: 0.8577\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkHdFNsaBdJoSIHy1ZE8sV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"88863592db9a4182b033310a64c1ec5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b1d750b536042c89f2d1e1ea6f994b1","IPY_MODEL_b0a57da38a394bb7af4545b9405b20ca","IPY_MODEL_3a1daac9d5aa406daa7d3280c5b27737"],"layout":"IPY_MODEL_13a1e7df126240008bedce8b2c4fe5d0"}},"2b1d750b536042c89f2d1e1ea6f994b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fbfdd4e438745109deafb2f4a62cac7","placeholder":"​","style":"IPY_MODEL_a3c3916a997b4a80ac21c6da10a251aa","value":"Downloading pytorch_model.bin: 100%"}},"b0a57da38a394bb7af4545b9405b20ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbf5ceafd597464f8220d6e7cdf7e771","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52f0f31fe19249c4a7c5fe2a0bf13e6a","value":440473133}},"3a1daac9d5aa406daa7d3280c5b27737":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47b4fa6f33ec4d038ac233f1def99c6e","placeholder":"​","style":"IPY_MODEL_c01ac64a14044e1c919c21b5d3303f1e","value":" 440M/440M [00:02&lt;00:00, 165MB/s]"}},"13a1e7df126240008bedce8b2c4fe5d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fbfdd4e438745109deafb2f4a62cac7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3c3916a997b4a80ac21c6da10a251aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbf5ceafd597464f8220d6e7cdf7e771":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52f0f31fe19249c4a7c5fe2a0bf13e6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47b4fa6f33ec4d038ac233f1def99c6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c01ac64a14044e1c919c21b5d3303f1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d7493199ef44caebde3a17c6f9020a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f836493c43214b96b74866b5f7cd6398","IPY_MODEL_1da9681bdd4643b9b4f948c22cacb835","IPY_MODEL_e0d86976dac246f0b8772e0d92e6dc8a"],"layout":"IPY_MODEL_2645e93391cd4cdea171f58bbc8ee491"}},"f836493c43214b96b74866b5f7cd6398":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70d6f9196d274118ad4efd20aa6b50a8","placeholder":"​","style":"IPY_MODEL_f747ea9342824ac2a4dd930df2fd0134","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"1da9681bdd4643b9b4f948c22cacb835":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0f8c88d13e4c849c0f967fc7f321c3","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93b1ac6fdfca4f4f9d1bbffc2bbfda3a","value":231508}},"e0d86976dac246f0b8772e0d92e6dc8a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25a2c54c96b545f88fa5030497636f35","placeholder":"​","style":"IPY_MODEL_1770b70ae18741058a3d1dda11022e8b","value":" 232k/232k [00:00&lt;00:00, 4.19MB/s]"}},"2645e93391cd4cdea171f58bbc8ee491":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70d6f9196d274118ad4efd20aa6b50a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f747ea9342824ac2a4dd930df2fd0134":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e0f8c88d13e4c849c0f967fc7f321c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93b1ac6fdfca4f4f9d1bbffc2bbfda3a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25a2c54c96b545f88fa5030497636f35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1770b70ae18741058a3d1dda11022e8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06aabf3ec7964d70aa654ee98459abad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06b66cc5c63f404881224bf742016940","IPY_MODEL_2c4f6250dae0436ba869bc69b4fbc01e","IPY_MODEL_7ca9d3d97f9f430e87743ca5b5a70335"],"layout":"IPY_MODEL_3b2b5c2977104f04a2436a1d988d03e5"}},"06b66cc5c63f404881224bf742016940":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fb68a267b5d445ba16e977b037f09c3","placeholder":"​","style":"IPY_MODEL_6b928b28f8d045c6b49c1a30d82f24ae","value":"Downloading (…)okenizer_config.json: 100%"}},"2c4f6250dae0436ba869bc69b4fbc01e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b447cea33fa471fb7555f1c65ea9903","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0deef85ac6cc42f89c6ec91886401895","value":28}},"7ca9d3d97f9f430e87743ca5b5a70335":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8836a0373aa64c8e97fdef01aa2ad5d3","placeholder":"​","style":"IPY_MODEL_55475b7453cc4b449bab8265e59f174e","value":" 28.0/28.0 [00:00&lt;00:00, 858B/s]"}},"3b2b5c2977104f04a2436a1d988d03e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb68a267b5d445ba16e977b037f09c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b928b28f8d045c6b49c1a30d82f24ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b447cea33fa471fb7555f1c65ea9903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0deef85ac6cc42f89c6ec91886401895":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8836a0373aa64c8e97fdef01aa2ad5d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55475b7453cc4b449bab8265e59f174e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b38d33f9f81945f7bd998f9da9a64203":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bdc8a4473f4479fbb2fca4fe32ee3ce","IPY_MODEL_3feb2a7dff6143d29c6a1724aa364afd","IPY_MODEL_5834a9609ab64fac872f0e671d64ed90"],"layout":"IPY_MODEL_45ed757cdb3e44b9ba059e457b71f0c7"}},"1bdc8a4473f4479fbb2fca4fe32ee3ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_475c434195ed47b1a5f64c20534ccb50","placeholder":"​","style":"IPY_MODEL_9372524ce8e544aeb3540c60ae1e6709","value":"Downloading (…)lve/main/config.json: 100%"}},"3feb2a7dff6143d29c6a1724aa364afd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc24b9e862e849b98a9bb59715934cc0","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d62bb059e8734cee951a74d663cd8650","value":570}},"5834a9609ab64fac872f0e671d64ed90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98f7b3168d014bbc889750b0297c9dcc","placeholder":"​","style":"IPY_MODEL_d12df694e4f64923ad54d15a47ece828","value":" 570/570 [00:00&lt;00:00, 8.79kB/s]"}},"45ed757cdb3e44b9ba059e457b71f0c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"475c434195ed47b1a5f64c20534ccb50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9372524ce8e544aeb3540c60ae1e6709":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc24b9e862e849b98a9bb59715934cc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d62bb059e8734cee951a74d663cd8650":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98f7b3168d014bbc889750b0297c9dcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d12df694e4f64923ad54d15a47ece828":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}